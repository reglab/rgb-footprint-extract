{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12a61d4b",
   "metadata": {},
   "source": [
    "# Functions to run the polygon pipeline for ADU permit-matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c48ca7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import rasterio\n",
    "from rasterio.transform import from_bounds\n",
    "from rasterio import plot\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Inference functions\n",
    "%run inference-functions.ipynb import get_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d544066c",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbb1e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sources():\n",
    "    tif_fp = {\n",
    "    '2016': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/2016/raw_tif', \n",
    "    '2018': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/2018/raw_tif',\n",
    "    '2020': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/raw_tif'\n",
    "    }\n",
    "    inferences_dir = {\n",
    "        '2016': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/2016/infer',\n",
    "        '2018': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/2018/infer',\n",
    "        '2020': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/phase2_superresx2/infer/'\n",
    "    }\n",
    "    img_fp = {\n",
    "        '2016': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/2016/superresx2',\n",
    "        '2018': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/2018/superresx2',\n",
    "        '2020': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/phase2_superresx2'\n",
    "    }\n",
    "    return tif_fp, inferences_dir, img_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86749684",
   "metadata": {},
   "source": [
    "## Process polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea5798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_OSM_data(inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, osm_buildings_parcel,\n",
    "                     min_area_thresh, flatten_threshold):\n",
    "    # Returns a gpd.GeoDataFrame with the following columns:\n",
    "    #   - GEOID, area, small, large\n",
    "    #   - flags: OSM_flag, expansion_OSM_flag, main_building_flag, expansion_2016_flag\n",
    "    #   - building geometry\n",
    "    gpd_cols = ['main_building_flag', 'OSM_flag', 'expansion_OSM_flag', 'expansion_2016_flag', 'geometry']\n",
    "    parcel_buildings = gpd.GeoDataFrame(geometry=[], columns=gpd_cols)\n",
    "    \n",
    "    # Drop OSM index_left column\n",
    "    if osm_buildings_parcel is not None and 'index_left' in osm_buildings_parcel.columns:\n",
    "        osm_buildings_parcel.drop('index_left', axis=1, inplace=True)\n",
    "        \n",
    "    # Identify main buildings\n",
    "    parcel_builds, parcel_main_geoms = identify_main_buildings(\n",
    "        inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, osm_buildings_parcel)\n",
    "    ib_2020_parcel, ib_2016_parcel, osm_parcel = parcel_builds\n",
    "    ib_2020_main_geom, ib_2016_main_geom, osm_main_geom = parcel_main_geoms\n",
    "    \n",
    "    # Process the 2016 inferences\n",
    "    # Note: We process 2016 in a special manner. We do not default to OSM as we do for\n",
    "    # 2020, but rather just use OSM to complete the predictions that were inferred\n",
    "    # by the model. \n",
    "    \n",
    "    # Merge buildings with OSM annotations\n",
    "    ib_2016_parcel = merge_buildings(\n",
    "        gdf=ib_2016_parcel, comp=osm_parcel, \n",
    "        area_threshold_expansion=min_area_thresh, flatten_threshold=flatten_threshold,\n",
    "        limit_to_inferences=True)\n",
    "    \n",
    "    # Case 1: No inference nor OSM data ----------------- \n",
    "    if ib_2020_parcel is None and osm_parcel is None:\n",
    "        return parcel_buildings [gpd_cols], np.NAN\n",
    "\n",
    "    \n",
    "    # Case 2: No inference but OSM data ----------------- \n",
    "    # We fully rely on OSM data\n",
    "    if ib_2020_parcel is None and osm_parcel is not None:\n",
    "        parcel_buildings = osm_parcel.copy()\n",
    "        parcel_buildings['OSM_flag'] = True\n",
    "        \n",
    "        # Compare to 2016 footprints\n",
    "        parcel_buildings = compare_buildings(\n",
    "            gdf=parcel_buildings, comp_list=[ib_2016_parcel], name_list=['2016'], \n",
    "            area_threshold_expansion=min_area_thresh, flatten_threshold=flatten_threshold)\n",
    "        parcel_buildings['expansion_OSM_flag'] = False\n",
    "        \n",
    "        \n",
    "    # Case 3: Inference and no OSM data ----------------- \n",
    "    if osm_parcel is None and ib_2020_parcel is not None:\n",
    "        \n",
    "        # Compare to 2016 footprints\n",
    "        parcel_buildings = compare_buildings(\n",
    "            gdf=ib_2020_parcel, comp_list=[ib_2016_parcel], name_list=['2016'],  \n",
    "            area_threshold_expansion=min_area_thresh, flatten_threshold=flatten_threshold\n",
    "            )\n",
    "        \n",
    "        # Reflect lack of OSM\n",
    "        parcel_buildings['OSM_flag'] = False\n",
    "        parcel_buildings['expansion_OSM_flag'] = None\n",
    "\n",
    "        \n",
    "    # Case 4: Inference and OSM data -----------------   \n",
    "    \n",
    "    if osm_parcel is not None and ib_2020_parcel is not None:\n",
    "        # Merge with OSM footprints\n",
    "        ib_2020_parcel = merge_buildings(\n",
    "            gdf=ib_2020_parcel, comp=osm_parcel, \n",
    "            area_threshold_expansion=min_area_thresh, flatten_threshold=flatten_threshold,\n",
    "            limit_to_inferences=False)\n",
    "        \n",
    "        # Check for 2016 and OSM expansions\n",
    "        parcel_buildings = compare_buildings(\n",
    "            gdf=ib_2020_parcel, comp_list=[osm_parcel, ib_2016_parcel], name_list=['OSM', '2016'],  \n",
    "            area_threshold_expansion=min_area_thresh, flatten_threshold=flatten_threshold\n",
    "            )\n",
    "        \n",
    "        # Raw main building expansion check for 2020 vs 2016\n",
    "        if inferred_buildings_2020_parcel is not None and inferred_buildings_2016_parcel is not None:\n",
    "            raw_main_2020 = inferred_buildings_2020_parcel.sort_values('area', ascending=False).iloc[0]['geometry']\n",
    "            raw_main_2016 = inferred_buildings_2016_parcel.sort_values('area', ascending=False).iloc[0]['geometry']\n",
    "            main_exp = (raw_main_2020.intersection(raw_main_2016)).area / (raw_main_2020.union(raw_main_2016)).area\n",
    "            if main_exp < 0.8:\n",
    "                parcel_buildings.loc[(parcel_buildings['main_building_flag'] == True), 'expansion_2016_flag'] = True\n",
    "        \n",
    "    # Compute building area\n",
    "    parcel_buildings['area'] = parcel_buildings.to_crs('EPSG:26910').geometry.area\n",
    "    \n",
    "    # Generate data dict (for debugging)\n",
    "    data_dict = {'2020_output': ib_2020_parcel, '2016_output': ib_2016_parcel, 'osm_output': osm_parcel}\n",
    "    return parcel_buildings[gpd_cols + ['area']], data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0016bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_buildings(gdf, comp_list, name_list, area_threshold_expansion, flatten_threshold\n",
    "                    ):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    match_cols = ['GEOID', 'area', 'iou', 'main_building_flag',\n",
    "                  'OSM_flag', 'geometry'] +  ['expansion_{}_flag'.format(comp_name) for comp_name in name_list]\n",
    "    \n",
    "    if gdf is None:\n",
    "        return None\n",
    "    \n",
    "    for comp_name in name_list:\n",
    "        gdf['expansion_{}_flag'.format(comp_name)] = None\n",
    "    \n",
    "    for comp, comp_name in zip(comp_list, name_list):\n",
    "        \n",
    "        if comp is None:\n",
    "            continue\n",
    "        \n",
    "        # Check expansion\n",
    "        if len(gdf) > 0:\n",
    "            comp['geometry_comp'] = comp['geometry']\n",
    "\n",
    "            comp = comp.reset_index()\n",
    "            gdf = gdf.sjoin(comp[['geometry', 'geometry_comp']], how='left', predicate='intersects')\n",
    "            gdf['iou_comp'] = gdf.apply(lambda row: (\n",
    "                row['geometry'].intersection(comp.iloc[int(row['index_right'])]['geometry'])).area / \n",
    "                                    row['geometry'].area if pd.notnull(row['index_right']) else None, axis=1)\n",
    "\n",
    "            # Have to account for potentially various matches for one inference\n",
    "            gdf = gdf.sort_values('iou_comp', ascending=False)\n",
    "            gdf.drop_duplicates(subset=['geometry'], keep='first', inplace=True)\n",
    "\n",
    "            # Compare expansion\n",
    "            gdf['expansion_{}_flag'.format(comp_name)] = gdf.apply(\n",
    "                lambda row: compare_building_footprint(\n",
    "                    base_geom=comp.iloc[int(row['index_right'])]['geometry'], \n",
    "                    new_geom=row['geometry'].union(comp.iloc[int(row['index_right'])]['geometry']), \n",
    "                    diff_type='protruding_poly', \n",
    "                    area_threshold=area_threshold_expansion) if pd.notnull(row['index_right']) else True, \n",
    "                axis=1)\n",
    "\n",
    "        gdf = gdf[match_cols]\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7332ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_buildings(gdf, comp, area_threshold_expansion, flatten_threshold, limit_to_inferences):\n",
    "    match_cols = ['GEOID', 'area', 'iou', 'main_building_flag', 'OSM_flag', 'geometry'] \n",
    "    \n",
    "    if gdf is None:\n",
    "        return None\n",
    "    \n",
    "    if comp is None:\n",
    "        return gdf\n",
    "    \n",
    "    # Default to OSM\n",
    "    parcel_build = comp.copy()\n",
    "    \n",
    "    # Check for missing buildings in OSM annotations\n",
    "    parcel_union = parcel_build.geometry.unary_union\n",
    "    \n",
    "    fp = gdf.copy()\n",
    "\n",
    "    fp['inf_not_covered'] = fp.apply(\n",
    "    lambda row: compare_building_footprint(\n",
    "        base_geom=parcel_union, \n",
    "        new_geom=row['geometry'], \n",
    "        diff_type='protruding_poly', \n",
    "        area_threshold=area_threshold_expansion), \n",
    "    axis=1)\n",
    "\n",
    "    fp = fp.sjoin(parcel_build[['geometry']], how='left')\n",
    "    #print(fp)\n",
    "    fp = fp.loc[(fp['inf_not_covered'] == True) | (fp['index_right'].isna())]\n",
    "\n",
    "    # After this step, we have any building not covered by OSM annotations.\n",
    "    # This includes small buildings, or expanded buildings.\n",
    "\n",
    "    # Account for multiple OSM matches\n",
    "    fp.drop_duplicates(subset=['geometry'], inplace=True)\n",
    "\n",
    "    fp['OSM_flag'] = False\n",
    "    \n",
    "    # Keep only buildings identified in inferences (we're lenient and allow for anything)\n",
    "    # that is at least 30% covered by the inferences to be included.\n",
    "    if limit_to_inferences:\n",
    "        gdf_geom = gdf.geometry.unary_union\n",
    "        parcel_build['iou_gdf'] = parcel_build['geometry'].intersection(gdf_geom).area/parcel_build['geometry'].area\n",
    "        parcel_build = parcel_build.loc[parcel_build['iou_gdf'] > 0.3]\n",
    "        \n",
    "        #parcel_build = parcel_build.sjoin(gdf[['geometry']], how='left', predicate='intersects')\n",
    "        #parcel_build = parcel_build.loc[~parcel_build['index_right'].isna()]\n",
    "    \n",
    "    parcel_build = pd.concat([parcel_build[match_cols], fp[match_cols]])\n",
    "    \n",
    "    # Flatten\n",
    "    gdf = flatten_geometries(parcel_build, flatten_threshold) \n",
    "    return gdf\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef9d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_main_building(inference_buildings, osm_main_build):\n",
    "    \"\"\"\n",
    "    inference_buildings: (gpd.GeoDataFrame) parcel inference all buildings\n",
    "    osm_main_build: (gpd.GeoDataFrame) parcel OSM main building\n",
    "    \"\"\"\n",
    "    \n",
    "    if inference_buildings is None:\n",
    "        return None\n",
    "    \n",
    "    inference_buildings = inference_buildings.sort_values(\n",
    "            'area', ascending=False)\n",
    "    cols = ['GEOID', 'area',  'geometry', 'iou', 'OSM_flag', 'main_building_flag']\n",
    "    \n",
    "    # If OSM is unavailable, we extract the largest polygon\n",
    "    if osm_main_build is None:\n",
    "        inference_buildings = inference_buildings.reset_index(drop=True)\n",
    "        inference_buildings['main_building_flag'] = inference_buildings.apply(\n",
    "            lambda row: True if row.name == 0 else False, axis=1)\n",
    "        inference_buildings['OSM_flag'] = False\n",
    "    \n",
    "    # If OSM is available, we use the union between the polygons that overlap with \n",
    "    # the main building in OSM and the main building in OSM.\n",
    "    else:\n",
    "        # Identify inference buildings overlapping with OSM main building\n",
    "        inference_buildings = inference_buildings.sjoin(\n",
    "            osm_main_build[['geometry']], how='left', predicate='intersects')\n",
    "        inference_buildings['OSM_flag'] = False\n",
    "        \n",
    "        # If there is no overlap with the OSM main building, we use OSM.\n",
    "        if inference_buildings['index_right'].isna().mean() == 1:\n",
    "            inference_buildings['main_building_flag'] = False\n",
    "            inference_buildings = pd.concat([osm_main_build[cols], inference_buildings[cols]])\n",
    "            inference_buildings = inference_buildings.sort_values('area', ascending=False)\n",
    "        else:\n",
    "            inference_buildings['main_building_flag'] = inference_buildings['index_right'].apply(\n",
    "                lambda x: True if pd.notnull(x) else False)\n",
    "\n",
    "            # Combine main building polygon\n",
    "            inference_buildings['dissolve_idx'] = np.arange(len(inference_buildings))\n",
    "            inference_buildings['dissolve_idx'] = inference_buildings.apply(lambda row: 99 if row['main_building_flag'] else row['dissolve_idx'], axis=1)\n",
    "            inference_buildings = inference_buildings.dissolve(\n",
    "                by='dissolve_idx', aggfunc={\n",
    "                     \"area\": \"sum\",\n",
    "                     'GEOID': 'first',\n",
    "                     'iou': 'mean',\n",
    "                     'main_building_flag': 'max'\n",
    "                 },).reset_index()\n",
    "            inference_buildings.drop(['dissolve_idx'], axis=1, inplace=True)\n",
    "            inference_buildings = inference_buildings.sort_values('area', ascending=False)\n",
    "            inference_buildings = inference_buildings.reset_index(drop=True)\n",
    "            \n",
    "            # Replace with union of overlapping poly and OSM\n",
    "            inf_union = inference_buildings.iloc[0]['geometry'].union(\n",
    "                osm_main_build.geometry.unary_union)\n",
    "            inference_buildings.at[0, 'geometry'] = inf_union\n",
    "            inference_buildings['OSM_flag'] = False\n",
    "            \n",
    "    return inference_buildings[cols]\n",
    "\n",
    "def identify_main_buildings(inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, osm_buildings_parcel):\n",
    "    # Identify OSM main building\n",
    "    osm_main_build_geom, osm_main_build = None, None\n",
    "    \n",
    "    if osm_buildings_parcel is not None:\n",
    "        # Filter duplicate geometries\n",
    "        osm_buildings_parcel.drop_duplicates(subset=['geometry'], inplace=True)\n",
    "        \n",
    "        osm_buildings_parcel = osm_buildings_parcel.sort_values('area', ascending=False)\n",
    "        \n",
    "        # Identify main building\n",
    "        osm_buildings_parcel = osm_buildings_parcel.reset_index(drop=True)\n",
    "        osm_buildings_parcel['main_building_flag'] = osm_buildings_parcel.apply(\n",
    "            lambda row: True if row.name == 0 else False, axis=1)\n",
    "        \n",
    "        osm_buildings_parcel['OSM_flag'] = True\n",
    "        osm_main_build = osm_buildings_parcel.iloc[[0]]\n",
    "        osm_main_build_geom = osm_buildings_parcel.iloc[0]['geometry']\n",
    "        \n",
    "    # Identify main 2016 building\n",
    "    inferred_buildings_2016_main_geom = None\n",
    "    if inferred_buildings_2016_parcel is not None:\n",
    "        inferred_buildings_2016_parcel = inferred_buildings_2016_parcel.reset_index(drop=True)\n",
    "        inferred_buildings_2016_parcel = get_inference_main_building(\n",
    "            inferred_buildings_2016_parcel, osm_main_build)\n",
    "        inferred_buildings_2016_main_geom = inferred_buildings_2016_parcel.iloc[0]['geometry']\n",
    "    \n",
    "    # Identify main 2020 building\n",
    "    inferred_buildings_2020_main_geom = None\n",
    "    if inferred_buildings_2020_parcel is not None:\n",
    "        inferred_buildings_2020_parcel = inferred_buildings_2020_parcel.reset_index(drop=True)\n",
    "        inferred_buildings_2020_parcel = get_inference_main_building(\n",
    "                inferred_buildings_2020_parcel, osm_main_build)\n",
    "        inferred_buildings_2020_main_geom = inferred_buildings_2020_parcel.iloc[0]['geometry']\n",
    "\n",
    "    parcel_builds = inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, osm_buildings_parcel\n",
    "    parcel_main_geoms = inferred_buildings_2020_main_geom, inferred_buildings_2016_main_geom, osm_main_build_geom\n",
    "    \n",
    "    return parcel_builds, parcel_main_geoms\n",
    "\n",
    "\n",
    "\n",
    "def compute_largest_protruding_poly(union_build, base_build):\n",
    "    \"\"\"\n",
    "    Note we care about concentrated building\n",
    "    # expansions along a single wall and not general changes in building footprint due to noisy\n",
    "    # inferences so we isolate the largest protruding polygon\n",
    "    :return: (gpd.GeoSeries)\n",
    "    \"\"\"\n",
    "    diff_build = gpd.GeoDataFrame(geometry=[union_build.difference(base_build)], crs='EPSG:4326')\n",
    "\n",
    "    # Break up polygons\n",
    "    diff_build = diff_build.to_crs('EPSG:26910')\n",
    "    diff_build['geometry'] = diff_build.geometry.buffer(-0.2)\n",
    "    diff_build = diff_build.explode(ignore_index=True, index_parts=False)\n",
    "    diff_build['geometry'] = diff_build.geometry.buffer(0.2)\n",
    "    diff_build = diff_build.to_crs('EPSG:4326')\n",
    "\n",
    "    # Return largest polygon\n",
    "    diff_build['area'] = diff_build.to_crs('EPSG:26910').area\n",
    "    diff_build = diff_build.sort_values('area', ascending=False).iloc[0]\n",
    "    \n",
    "    return diff_build\n",
    "  \n",
    "    \n",
    "def compare_building_footprint(base_geom, new_geom, diff_type, area_threshold):\n",
    "    expansion_flag = None\n",
    "    if base_geom is not None:\n",
    "        expansion_flag = False\n",
    "\n",
    "        if diff_type == 'protruding_poly':\n",
    "            diff_gpd = compute_largest_protruding_poly(new_geom, base_geom)\n",
    "        elif diff_type == 'raw_poly':\n",
    "            raise Exception('[ERROR] Raw poly comparison not implemented.')\n",
    "\n",
    "        if diff_gpd['area'] > area_threshold:\n",
    "            expansion_flag = True\n",
    "    return expansion_flag\n",
    "\n",
    "\n",
    "def flatten_geometries(gdf, threshold):\n",
    "    def check_overlapping_polygons(df, row):\n",
    "        unique = True\n",
    "        for i in set(range(len(df))).difference(set([row.name])):\n",
    "            intersect = ((row['geometry'].intersection(df.to_crs('EPSG:26910').iloc[i].geometry)).area) / row['geometry'].area\n",
    "            if intersect > threshold:\n",
    "                unique = False\n",
    "        return unique\n",
    "    \n",
    "    gdf = gdf.copy()\n",
    "    gdf = gdf.reset_index(drop=True)\n",
    "    \n",
    "    gdf['unique'] = gdf.to_crs('EPSG:26910').apply(\n",
    "        lambda row: check_overlapping_polygons(gdf, row), axis=1)\n",
    "    \n",
    "    # Get unique geometries\n",
    "    gdf = gdf.loc[(gdf['unique'] == True) | ((gdf['main_building_flag'] == True) & (gdf['OSM_flag'] == False))]\n",
    "    \n",
    "    # Recompute area\n",
    "    gdf['area'] = gdf.to_crs('EPSG:26910').geometry.area\n",
    "    gdf = gdf.sort_values('area', ascending=False)\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac99f001",
   "metadata": {},
   "source": [
    "## Plotting tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bea108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_osm_apn(parcel_apn, area_threshold, flatten_threshold):\n",
    "    inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, osm_buildings_parcel, permits_parcel = parcel_level_data(parcel_apn, sj_parcels_res, sj_parcel_permit)\n",
    "    parcel_bounds = sj_parcels_res[sj_parcels_res['APN'] == parcel_apn]\n",
    "\n",
    "    # Incorporate OSM data\n",
    "    parcel_buildings, _ = process_OSM_data(\n",
    "        inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, osm_buildings_parcel, area_threshold, flatten_threshold)\n",
    "\n",
    "    # Plot\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(18, 8))\n",
    "    parcel_bounds.plot(ax=ax1, edgecolor='black', facecolor='none')\n",
    "    if osm_buildings_parcel is not None:\n",
    "        osm_buildings_parcel.plot(ax=ax1, color='blue', alpha=0.7)\n",
    "    if inferred_buildings_2020_parcel is not None:\n",
    "        inferred_buildings_2020_parcel.plot(ax=ax1, color='red', alpha=0.7)\n",
    "    ax1.axis('off')\n",
    "\n",
    "    parcel_bounds.plot(ax=ax2, edgecolor='black', facecolor='none')\n",
    "    if osm_buildings_parcel is not None:\n",
    "        osm_buildings_parcel.plot(ax=ax2, color='blue', alpha=0.7)\n",
    "    if inferred_buildings_2016_parcel is not None:\n",
    "        inferred_buildings_2016_parcel.plot(ax=ax2, color='red', alpha=0.7)\n",
    "    ax2.axis('off')\n",
    "\n",
    "    parcel_bounds.plot(ax=ax3, edgecolor='black', facecolor='none')\n",
    "    parcel_buildings.plot(ax=ax3, color='blue', alpha=0.7)\n",
    "    if osm_buildings_parcel is not None:\n",
    "        osm_buildings_parcel.plot(ax=ax3, color='blue', alpha=0)\n",
    "    ax3.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return parcel_buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ce58b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_osm_apn_full_detail(parcel_apn, area_threshold, flatten_threshold, imagery=None):\n",
    "    inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, osm_buildings_parcel, permits_parcel = parcel_level_data(parcel_apn, sj_parcels_res, sj_parcel_permit)\n",
    "    parcel_bounds = sj_parcels_res[sj_parcels_res['APN'] == parcel_apn]\n",
    "\n",
    "    # Incorporate OSM data\n",
    "    parcel_buildings, data_dict = process_OSM_data(\n",
    "        inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, \n",
    "        osm_buildings_parcel, area_threshold, flatten_threshold)\n",
    "\n",
    "    ib_2020_parcel = data_dict['2020_output']\n",
    "    ib_2016_parcel = data_dict['2016_output']\n",
    "    osm_parcel = data_dict['osm_output']\n",
    "    \n",
    "    # Plot\n",
    "    if imagery is None:\n",
    "        fig, axs = plt.subplots(ncols=3, nrows=2, figsize=(18, 10))\n",
    "        (ax1, ax2, ax3), (ax4, ax5, ax6) = axs\n",
    "    else:\n",
    "        fig, axs = plt.subplots(ncols=3, nrows=3, figsize=(18, 15))\n",
    "        (ax1, ax2, ax3), (ax4, ax5, ax6), (ax7, ax8, ax9) = axs\n",
    "    \n",
    "    for ax in [a for alist in axs for a in alist]:\n",
    "        parcel_bounds.plot(ax=ax, edgecolor='black', facecolor='none')\n",
    "        ax.axis('off')\n",
    "        \n",
    "    for ax in (ax1, ax2):\n",
    "        if osm_buildings_parcel is not None:\n",
    "            osm_buildings_parcel.plot(ax=ax, color='blue', alpha=0.3)\n",
    "            \n",
    "    if inferred_buildings_2020_parcel is not None:\n",
    "        inferred_buildings_2020_parcel.plot(ax=ax1, color='red', alpha=0.7)\n",
    "    ax1.set_title('2020 inferences (raw)')\n",
    "        \n",
    "    if inferred_buildings_2016_parcel is not None:\n",
    "        inferred_buildings_2016_parcel.plot(ax=ax2, color='red', alpha=0.7)\n",
    "    ax2.set_title('2016 inferences (raw)')\n",
    "        \n",
    "    if osm_parcel is not None:\n",
    "        osm_parcel.plot(ax=ax3, color='blue', alpha=0.7)\n",
    "    ax3.set_title('OSM annotations')\n",
    "        \n",
    "    if ib_2020_parcel is not None:\n",
    "        ib_2020_parcel.plot(ax=ax4, color='red', alpha=0.7)\n",
    "    ax4.set_title('OSM-adjusted 2020 polygons')\n",
    "    \n",
    "    if ib_2016_parcel is not None:\n",
    "        ib_2016_parcel.plot(ax=ax5, color='red', alpha=0.7)\n",
    "    ax5.set_title('OSM-adjusted 2016 polygons')\n",
    "    \n",
    "    # Output\n",
    "    parcel_buildings.plot(ax=ax6, color='purple', alpha=0.7)\n",
    "    ax6.set_title('Output')\n",
    "    \n",
    "    # Satellite images\n",
    "    if imagery is not None:\n",
    "        \n",
    "        for year, ax in zip(['2020', '2016'], (ax7, ax8)):\n",
    "            # Get imagery\n",
    "            file_name = get_file_name_from_parcel(\n",
    "                parcel_apn, imagery['sj_parcels_res'], imagery['tiles_gdf'][year])\n",
    "            img_file, superres_file = find_image_file_and_superrestile(\n",
    "                imagery['img_fp'][year], imagery['tif_fp'][year], file_name)\n",
    "\n",
    "            with rasterio.open(superres_file) as src:\n",
    "                out_image, out_transform = rasterio.mask.mask(\n",
    "                    src, parcel_bounds.to_crs('EPSG:26910')['geometry'], crop=True)\n",
    "            \n",
    "            # Plot\n",
    "            rasterio.plot.show(out_image, transform=out_transform, ax=ax)\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    return parcel_buildings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e388ed6",
   "metadata": {},
   "source": [
    "## Visualizing satellite imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ba82738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name_from_parcel(parcel_apn, sj_parcels_res, tiles_gdf_year):\n",
    "    # Get parcel bounds\n",
    "    parcel_bounds = sj_parcels_res[sj_parcels_res['APN'] == parcel_apn]\n",
    "    \n",
    "    # Get tiles\n",
    "    tiles = tiles_gdf_year.copy()\n",
    "    \n",
    "    # Return tile with largest overlap with parcel\n",
    "    tiles['iou'] = tiles['geometry'].intersection(parcel_bounds.iloc[0]['geometry']).area\n",
    "    tiles = tiles.sort_values('iou', ascending=False)\n",
    "    tiles = tiles.iloc[0]\n",
    "    \n",
    "    return tiles['file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53b308c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_image_file_and_superrestile(img_fp, tif_fp, file_name):\n",
    "    \"\"\"\n",
    "    Searches for the inference file within train, val and test directories.\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.join(img_fp, 'train')):\n",
    "        # For 2020 data which is split across train, val and test\n",
    "        img_file = None\n",
    "        for dirname in ['train', 'val', 'test']:\n",
    "            dirpath = os.path.join(img_fp, dirname, 'images', '{}.npy'.format(file_name))\n",
    "            if os.path.exists(dirpath):\n",
    "                img_file = dirpath\n",
    "    else:\n",
    "        # For 2016 and 2018 data which is not split\n",
    "        img_file = os.path.join(img_fp, '{}.npy'.format(file_name))\n",
    "        \n",
    "    # Generate file \n",
    "    superres_tile = os.path.join(img_fp, '..', 'superres_tif', '{}.tif'.format(file_name))\n",
    "    if not os.path.exists(superres_tile):\n",
    "        if not os.path.exists(os.path.join(img_fp, '..', 'superres_tif')):\n",
    "            os.makedirs(os.path.join(img_fp, '..', 'superres_tif'))\n",
    "        \n",
    "        tile_img = np.load(img_file).astype(np.uint8)\n",
    "        \n",
    "        # Get original raster\n",
    "        raster_original = rasterio.open(os.path.join(tif_fp, '{}.tif'.format(file_name)))\n",
    "        t = from_bounds(*raster_original.bounds, tile_img.shape[0], tile_img.shape[1])\n",
    "        raster_crs = rasterio.crs.CRS({\"init\": \"epsg:26910\"})\n",
    "        \n",
    "        with rasterio.open(superres_tile, 'w', driver='GTiff', \n",
    "                           height=tile_img.shape[0], width=tile_img.shape[1],\n",
    "                           count=3, dtype=str(tile_img.dtype),\n",
    "                           crs=raster_crs, transform=t) as raster_new:\n",
    "            raster_new.write(tile_img[:, :, 0], 1)\n",
    "            raster_new.write(tile_img[:, :, 1], 2)\n",
    "            raster_new.write(tile_img[:, :, 2], 3)\n",
    "            raster_new.close()\n",
    "        \n",
    "    return img_file, superres_tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2646b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tile_dicts_all_years(oak_fp, inferences_dir):\n",
    "    # Load tile dict for each year\n",
    "    tile_bounds_dict_all = {}\n",
    "    for year in ['2016', '2018', '2020']:\n",
    "        output_fp = os.path.join(oak_fp, 'outputs', 'cbg-inference-{}'.format(year))\n",
    "        with open(os.path.join(output_fp, 'tile_bounds.json'), \"r\") as f:\n",
    "            tile_bounds_dict = json.load(f)\n",
    "            tile_bounds_dict_all[year] = tile_bounds_dict\n",
    "    \n",
    "    # Get tiles for all years\n",
    "    tiles_gdf = {}\n",
    "    for year in ['2016', '2018', '2020']:\n",
    "        tiles = glob.glob(os.path.join(inferences_dir[year], '*.npy'))\n",
    "        tiles = [t.split(os.path.sep)[-1].replace('.npy', '') for t in tiles]\n",
    "        tile_metrics_pd = pd.DataFrame(tiles, columns=['file'])\n",
    "\n",
    "        tile_metrics_pd['geometry'] = tile_metrics_pd.file.progress_apply(\n",
    "            lambda name: get_bounds(tile_bounds_dict_all[year], name) if name in list(tile_bounds_dict_all[year].keys()) else None\n",
    "        )\n",
    "        tiles_gdf[year] = gpd.GeoDataFrame(tile_metrics_pd.copy(), crs='EPSG:4326')\n",
    "        \n",
    "    return tile_bounds_dict_all, tiles_gdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
