{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12a61d4b",
   "metadata": {},
   "source": [
    "# Functions to run the polygon pipeline for ADU permit-matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be8dc8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import rasterio\n",
    "from rasterio.transform import from_bounds\n",
    "from rasterio import plot\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Inference functions\n",
    "%run inference-functions.ipynb import get_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fada068",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec01611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sources():\n",
    "    tif_fp = {\n",
    "    '2016': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/2016/raw_tif', \n",
    "    '2018': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/2018/raw_tif',\n",
    "    '2020': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/raw_tif'\n",
    "    }\n",
    "    inferences_dir = {\n",
    "        '2016': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/2016/infer',\n",
    "        '2018': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/2018/infer',\n",
    "        '2020': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/phase2_superresx2/infer/'\n",
    "    }\n",
    "    img_fp = {\n",
    "        '2016': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/2016/superresx2',\n",
    "        '2018': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/2018/superresx2',\n",
    "        '2020': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/phase2_superresx2'\n",
    "    }\n",
    "    return tif_fp, inferences_dir, img_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6aaec7",
   "metadata": {},
   "source": [
    "## Process polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea5798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_OSM_data(inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, osm_buildings_parcel,\n",
    "                     min_area_thresh, flatten_threshold, parcel_bounds):\n",
    "    # Returns a gpd.GeoDataFrame with the following columns:\n",
    "    #   - GEOID, area, small, large\n",
    "    #   - flags: OSM_flag, expansion_OSM_flag, main_building_flag, expansion_2016_flag\n",
    "    #   - building geometry\n",
    "    gpd_cols = [\n",
    "        'main_building_flag', 'OSM_flag', 'expansion_OSM_flag', 'diff_OSM_value',\n",
    "        'expansion_2016_flag', 'diff_2016_value', 'geometry']\n",
    "    parcel_buildings = gpd.GeoDataFrame(geometry=[], columns=gpd_cols)\n",
    "    \n",
    "    # Drop OSM index_left column\n",
    "    if osm_buildings_parcel is not None and 'index_left' in osm_buildings_parcel.columns:\n",
    "        osm_buildings_parcel.drop('index_left', axis=1, inplace=True)\n",
    "        \n",
    "    # Identify main buildings\n",
    "    parcel_builds, parcel_main_geoms = identify_main_buildings(\n",
    "        inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, osm_buildings_parcel, parcel_bounds)\n",
    "    ib_2020_parcel, ib_2016_parcel, osm_parcel = parcel_builds\n",
    "    ib_2020_main_geom, ib_2016_main_geom, osm_main_geom = parcel_main_geoms\n",
    "    \n",
    "    # Process the 2016 inferences\n",
    "    # Note: We process 2016 in a special manner. We do not default to OSM as we do for\n",
    "    # 2020, but rather just use OSM to complete the predictions that were inferred\n",
    "    # by the model. \n",
    "    \n",
    "    # Merge buildings with OSM annotations\n",
    "    ib_2016_parcel = merge_buildings(\n",
    "        gdf=ib_2016_parcel, comp=osm_parcel, \n",
    "        area_threshold_expansion=min_area_thresh, flatten_threshold=flatten_threshold,\n",
    "        limit_to_inferences=True)\n",
    "    \n",
    "    # Case 1: No inference nor OSM data ----------------- \n",
    "    if ib_2020_parcel is None and osm_parcel is None:\n",
    "        return parcel_buildings[gpd_cols], np.NAN\n",
    "\n",
    "    \n",
    "    # Case 2: No inference but OSM data ----------------- \n",
    "    # We fully rely on OSM data\n",
    "    if ib_2020_parcel is None and osm_parcel is not None:\n",
    "        parcel_buildings = osm_parcel.copy()\n",
    "        parcel_buildings['OSM_flag'] = True\n",
    "        \n",
    "        # Compare to 2016 footprints\n",
    "        parcel_buildings = compare_buildings(\n",
    "            gdf=parcel_buildings, comp_list=[ib_2016_parcel], name_list=['2016'], \n",
    "            area_threshold_expansion=min_area_thresh, flatten_threshold=flatten_threshold)\n",
    "        parcel_buildings['expansion_OSM_flag'] = False\n",
    "        parcel_buildings['diff_OSM_value'] = 0\n",
    "        \n",
    "        \n",
    "    # Case 3: Inference and no OSM data ----------------- \n",
    "    if osm_parcel is None and ib_2020_parcel is not None:\n",
    "        \n",
    "        # Compare to 2016 footprints\n",
    "        parcel_buildings = compare_buildings(\n",
    "            gdf=ib_2020_parcel, comp_list=[ib_2016_parcel], name_list=['2016'],  \n",
    "            area_threshold_expansion=min_area_thresh, flatten_threshold=flatten_threshold\n",
    "            )\n",
    "        \n",
    "        # Reflect lack of OSM\n",
    "        parcel_buildings['OSM_flag'] = False\n",
    "        parcel_buildings['expansion_OSM_flag'] = None\n",
    "        parcel_buildings['diff_OSM_value'] = None\n",
    "\n",
    "        \n",
    "    # Case 4: Inference and OSM data -----------------   \n",
    "    if osm_parcel is not None and ib_2020_parcel is not None:\n",
    "        # Merge with OSM footprints\n",
    "        ib_2020_parcel = merge_buildings(\n",
    "            gdf=ib_2020_parcel, comp=osm_parcel, \n",
    "            area_threshold_expansion=min_area_thresh, flatten_threshold=flatten_threshold,\n",
    "            limit_to_inferences=False)\n",
    "        \n",
    "        # Check for 2016 and OSM expansions\n",
    "        parcel_buildings = compare_buildings(\n",
    "            gdf=ib_2020_parcel, comp_list=[osm_parcel, ib_2016_parcel], name_list=['OSM', '2016'],  \n",
    "            area_threshold_expansion=min_area_thresh, flatten_threshold=flatten_threshold\n",
    "            )\n",
    "        \n",
    "        # Raw main building expansion check for 2020 vs 2016\n",
    "        #if ib_2020_parcel is not None and ib_2016_parcel is not None:\n",
    "        #    raw_main_2020 = inferred_buildings_2020_parcel.sort_values('area', ascending=False).iloc[0]['geometry']\n",
    "        #    raw_main_2016 = inferred_buildings_2016_parcel.sort_values('area', ascending=False).iloc[0]['geometry']\n",
    "        #    main_exp = (raw_main_2020.intersection(raw_main_2016)).area / (raw_main_2020.union(raw_main_2016)).area\n",
    "        #    if main_exp < 0.8:\n",
    "        #        parcel_buildings.loc[(parcel_buildings['main_building_flag'] == True), 'expansion_2016_flag'] = True\n",
    "        \n",
    "    # Compute building area\n",
    "    parcel_buildings['area'] = parcel_buildings.to_crs('EPSG:26910').geometry.area\n",
    "    \n",
    "    # Generate data dict (for debugging)\n",
    "    data_dict = {'2020_output': ib_2020_parcel, '2016_output': ib_2016_parcel, 'osm_output': osm_parcel}\n",
    "    return parcel_buildings[gpd_cols + ['area']], data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0016bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_buildings(gdf, comp_list, name_list, area_threshold_expansion, flatten_threshold\n",
    "                    ):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    match_cols = ['GEOID', 'area', 'iou', 'main_building_flag',\n",
    "                  'OSM_flag', 'geometry'] +  [\n",
    "        'expansion_{}_flag'.format(comp_name) for comp_name in name_list] + [\n",
    "        'diff_{}_value'.format(comp_name) for comp_name in name_list]\n",
    "    \n",
    "    if gdf is None:\n",
    "        return None\n",
    "    \n",
    "    for comp_name in name_list:\n",
    "        gdf['expansion_{}_flag'.format(comp_name)] = None\n",
    "        gdf['diff_{}_value'.format(comp_name)] = None\n",
    "    \n",
    "    for comp, comp_name in zip(comp_list, name_list):\n",
    "        \n",
    "        if comp is None:\n",
    "            continue\n",
    "        \n",
    "        # Check expansion\n",
    "        if len(gdf) > 0:\n",
    "            comp['geometry_comp'] = comp['geometry']\n",
    "\n",
    "            comp = comp.reset_index()\n",
    "            gdf = gdf.sjoin(comp[['geometry', 'geometry_comp']], how='left', predicate='intersects')\n",
    "            gdf['iou_comp'] = gdf.apply(lambda row: (\n",
    "                row['geometry'].intersection(comp.iloc[int(row['index_right'])]['geometry'])).area / \n",
    "                                    row['geometry'].area if pd.notnull(row['index_right']) else None, axis=1)\n",
    "\n",
    "            # Have to account for potentially various matches for one inference\n",
    "            gdf = gdf.sort_values('iou_comp', ascending=False)\n",
    "            gdf.drop_duplicates(subset=['geometry'], keep='first', inplace=True)\n",
    "\n",
    "            # Compare expansion\n",
    "            gdf[['expansion_{}_flag'.format(comp_name), 'diff_{}_value'.format(comp_name)]] = gdf.apply(\n",
    "                lambda row: compare_building_footprint(\n",
    "                    base_geom=comp.iloc[int(row['index_right'])]['geometry'], \n",
    "                    new_geom=row['geometry'].union(comp.iloc[int(row['index_right'])]['geometry']), \n",
    "                    diff_type='protruding_poly', \n",
    "                    area_threshold=area_threshold_expansion) if pd.notnull(row['index_right']) else (True, True), \n",
    "                axis=1, result_type=\"expand\")\n",
    "            gdf.loc[gdf['diff_{}_value'.format(comp_name)] == True, 'diff_{}_value'.format(comp_name)] = gdf.loc[\n",
    "                gdf['diff_{}_value'.format(comp_name)] == True].to_crs('EPSG:26910')['geometry'].area\n",
    "\n",
    "        gdf = gdf[match_cols]\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7332ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_buildings(gdf, comp, area_threshold_expansion, flatten_threshold, limit_to_inferences):\n",
    "    match_cols = ['GEOID', 'area', 'iou', 'main_building_flag', 'OSM_flag', 'geometry'] \n",
    "    \n",
    "    if gdf is None:\n",
    "        return None\n",
    "    \n",
    "    if comp is None:\n",
    "        return gdf\n",
    "    \n",
    "    # Default to OSM for all buildings except main building\n",
    "    parcel_build = comp.loc[comp['main_building_flag'] == False].copy()\n",
    "    \n",
    "    # Check for missing buildings in OSM annotations\n",
    "    parcel_union = parcel_build.geometry.unary_union\n",
    "    \n",
    "    fp = gdf.loc[gdf['main_building_flag'] == False].copy()\n",
    "    if len(fp) > 0:\n",
    "        fp['inf_not_covered'] = fp.apply(\n",
    "            lambda row: compare_building_footprint(\n",
    "                base_geom=parcel_union, \n",
    "                new_geom=row['geometry'], \n",
    "                diff_type='protruding_poly', \n",
    "                area_threshold=area_threshold_expansion)[0], \n",
    "            axis=1)\n",
    "\n",
    "        fp = fp.sjoin(parcel_build[['geometry']], how='left')\n",
    "        fp = fp.loc[(fp['inf_not_covered'] == True) | (fp['index_right'].isna())]\n",
    "\n",
    "        # Account for multiple OSM matches\n",
    "        fp.drop_duplicates(subset=['geometry'], inplace=True)\n",
    "    fp['OSM_flag'] = False\n",
    "    \n",
    "    # Keep only buildings identified in inferences (we're lenient and allow for anything)\n",
    "    # that is at least 30% covered by the inferences to be included or \n",
    "    # inference footprint is 60% covered by OSM.\n",
    "\n",
    "    if limit_to_inferences and len(parcel_build) > 0:\n",
    "        # Cover 30% of OSM footprints\n",
    "        gdf_geom = gdf.geometry.unary_union\n",
    "        parcel_build['osm_coverage'] = parcel_build['geometry'].intersection(gdf_geom).area/parcel_build['geometry'].area\n",
    "        \n",
    "        # OSM covers 60% of inference footprint\n",
    "        gdf.reset_index(inplace=True, drop=True)\n",
    "        parcel_build = parcel_build.sjoin(gdf[['geometry']], how='left', predicate='intersects')\n",
    "        \n",
    "        parcel_build['inf_coverage'] = parcel_build.apply(\n",
    "            lambda row: 0 if pd.isnull(row['index_right']) else row['geometry'].intersection(\n",
    "                gdf.iloc[int(row['index_right'])]['geometry']).area/gdf.iloc[\n",
    "                int(row['index_right'])]['geometry'].area, axis=1)\n",
    "        parcel_build = parcel_build.loc[(parcel_build['osm_coverage'] > 0.3) | (parcel_build['inf_coverage'] > 0.6)]\n",
    "        \n",
    "    parcel_build = pd.concat([parcel_build[match_cols], fp[match_cols]])\n",
    "    \n",
    "    # Add back main building\n",
    "    parcel_build = pd.concat([gdf.loc[gdf['main_building_flag'] == True][match_cols], \n",
    "                              parcel_build[match_cols]])\n",
    "    \n",
    "    # Flatten\n",
    "    gdf = flatten_geometries(parcel_build, flatten_threshold)\n",
    "\n",
    "    return gdf\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23aa36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_gdf(gdf):\n",
    "    gdf = gdf.to_crs('EPSG:26910')\n",
    "    gdf['geometry'] = gdf['geometry'].simplify(tolerance=0.5, preserve_topology=True)\n",
    "    gdf = gdf.to_crs('EPSG:4326')\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef9d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_main_building(inference_buildings, osm_buildings_parcel):\n",
    "    \"\"\"\n",
    "    inference_buildings: (gpd.GeoDataFrame) parcel inference all buildings\n",
    "    osm_main_build: (gpd.GeoDataFrame) parcel OSM main building\n",
    "    \"\"\"\n",
    "    \n",
    "    if inference_buildings is None:\n",
    "        return None\n",
    "    \n",
    "    inference_buildings = inference_buildings.sort_values(\n",
    "            'area', ascending=False)\n",
    "    cols = ['GEOID', 'area',  'geometry', 'iou', 'OSM_flag', 'main_building_flag']\n",
    "    \n",
    "    # If OSM is unavailable, we extract the largest polygon\n",
    "    if osm_buildings_parcel is None:\n",
    "        inference_buildings = inference_buildings.reset_index(drop=True)\n",
    "        inference_buildings['main_building_flag'] = inference_buildings.apply(\n",
    "            lambda row: True if row.name == 0 else False, axis=1)\n",
    "        inference_buildings['OSM_flag'] = False\n",
    "        \n",
    "        # Simplify\n",
    "        inference_buildings = simplify_gdf(inference_buildings)\n",
    "        \n",
    "    # If OSM is available, we use the union between the polygons that overlap with \n",
    "    # the main building in OSM and the main building in OSM.\n",
    "    else:\n",
    "        # OSM main build\n",
    "        osm_main_build = osm_buildings_parcel.loc[osm_buildings_parcel['main_building_flag'] == True]\n",
    "        \n",
    "        # Identify inference buildings overlapping with OSM main building\n",
    "        inference_buildings = inference_buildings.sjoin(\n",
    "            osm_main_build[['geometry']], how='left', predicate='intersects')\n",
    "        inference_buildings['OSM_flag'] = False\n",
    "        \n",
    "        # If there is no overlap with the OSM main building, we use OSM.\n",
    "        if inference_buildings['index_right'].isna().mean() == 1:\n",
    "            inference_buildings['main_building_flag'] = False\n",
    "            \n",
    "            # Simplify small builds\n",
    "            inference_buildings = simplify_gdf(inference_buildings)\n",
    "            \n",
    "            inference_buildings = pd.concat([osm_main_build[cols], inference_buildings[cols]])\n",
    "            inference_buildings = inference_buildings.sort_values('area', ascending=False)\n",
    "        else:\n",
    "            inference_buildings['main_building_flag'] = inference_buildings['index_right'].apply(\n",
    "                lambda x: True if pd.notnull(x) else False)\n",
    "\n",
    "            # Combine main building polygon\n",
    "            inference_buildings['dissolve_idx'] = np.arange(len(inference_buildings))\n",
    "            inference_buildings['dissolve_idx'] = inference_buildings.apply(lambda row: 99 if row['main_building_flag'] else row['dissolve_idx'], axis=1)\n",
    "            inference_buildings = inference_buildings.dissolve(\n",
    "                by='dissolve_idx', aggfunc={\n",
    "                     \"area\": \"sum\",\n",
    "                     'GEOID': 'first',\n",
    "                     'iou': 'mean',\n",
    "                     'main_building_flag': 'max'\n",
    "                 },).reset_index()\n",
    "            inference_buildings.drop(['dissolve_idx'], axis=1, inplace=True)\n",
    "            inference_buildings = inference_buildings.sort_values('area', ascending=False)\n",
    "            inference_buildings = inference_buildings.reset_index(drop=True)\n",
    "            \n",
    "            # Simplify\n",
    "            inference_buildings = simplify_gdf(inference_buildings)\n",
    "            inference_buildings['OSM_flag'] = False\n",
    "            \n",
    "            # Replace with OSM coverage (as determined by the minimum rotated rectangle) plus\n",
    "            # whatever was not covered in OSM (potential expansions)\n",
    "            inf_main_build = inference_buildings.iloc[0]['geometry']\n",
    "            inf_mrr = inf_main_build.minimum_rotated_rectangle\n",
    "            inf_union = inf_mrr.intersection(osm_main_build.geometry.unary_union)\n",
    "            inf_union = inf_union.union(inf_main_build)\n",
    "            \n",
    "            # Replace with union of overlapping poly and OSM\n",
    "            #inf_union = inference_buildings.iloc[0]['geometry'].union(\n",
    "            #    osm_main_build.geometry.unary_union)\n",
    "            \n",
    "            # * Remove overlap with OSM small buildings\n",
    "            osm_small_build = osm_buildings_parcel.loc[osm_buildings_parcel['main_building_flag'] == False]\n",
    "            if len(osm_small_build) > 0:\n",
    "                inf_union_diff = inf_union.difference(osm_small_build.geometry.unary_union)\n",
    "                \n",
    "                # Add back the OSM small buildings we removed from main inference\n",
    "                if inf_union != inf_union_diff:\n",
    "                    covered_osm_buildings = osm_small_build.sjoin(gpd.GeoDataFrame(geometry=[inf_union]))\n",
    "                    covered_osm_buildings = covered_osm_buildings.loc[~covered_osm_buildings['index_right'].isna()]\n",
    "                    covered_osm_buildings['OSM_flag'] = True\n",
    "                    inference_buildings = pd.concat([inference_buildings[cols], covered_osm_buildings[cols]])\n",
    "                \n",
    "                inf_union = inf_union_diff\n",
    "                \n",
    "                # * Keep largest polygon (in case OSM difference broke up the polygon)\n",
    "                if type(inf_union) == shapely.geometry.multipolygon.MultiPolygon:\n",
    "                    inf_union = max(inf_union, key=lambda a: a.area)\n",
    "            \n",
    "            inference_buildings.at[0, 'geometry'] = inf_union\n",
    "            \n",
    "    return inference_buildings[cols]\n",
    "\n",
    "def identify_main_buildings(inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, \n",
    "                            osm_buildings_parcel, parcel_bounds):\n",
    "    # Identify OSM main building\n",
    "    osm_main_build_geom, osm_main_build = None, None\n",
    "    \n",
    "    if osm_buildings_parcel is not None:\n",
    "        # Filter duplicate geometries\n",
    "        osm_buildings_parcel.drop_duplicates(subset=['geometry'], inplace=True)\n",
    "        \n",
    "        osm_buildings_parcel = osm_buildings_parcel.sort_values('area', ascending=False)\n",
    "        \n",
    "        # Identify main building\n",
    "        osm_buildings_parcel = osm_buildings_parcel.reset_index(drop=True)\n",
    "        osm_buildings_parcel['main_building_flag'] = osm_buildings_parcel.apply(\n",
    "            lambda row: True if row.name == 0 else False, axis=1)\n",
    "        \n",
    "        osm_buildings_parcel['OSM_flag'] = True\n",
    "        osm_main_build_geom = osm_buildings_parcel.iloc[0]['geometry']\n",
    "        \n",
    "    # Identify main 2016 building\n",
    "    inferred_buildings_2016_main_geom = None\n",
    "    if inferred_buildings_2016_parcel is not None:\n",
    "        # Clip inferences\n",
    "        inferred_buildings_2016_parcel = gpd.clip(inferred_buildings_2016_parcel, parcel_bounds)\n",
    "        \n",
    "        inferred_buildings_2016_parcel = inferred_buildings_2016_parcel.reset_index(drop=True)\n",
    "        inferred_buildings_2016_parcel = get_inference_main_building(\n",
    "            inferred_buildings_2016_parcel, osm_buildings_parcel)\n",
    "        \n",
    "        inferred_buildings_2016_main_geom = inferred_buildings_2016_parcel.iloc[0]['geometry'] \n",
    "    \n",
    "    # Identify main 2020 building\n",
    "    inferred_buildings_2020_main_geom = None\n",
    "    if inferred_buildings_2020_parcel is not None:\n",
    "        # Clip inferences\n",
    "        inferred_buildings_2020_parcel = gpd.clip(inferred_buildings_2020_parcel, parcel_bounds)\n",
    "        \n",
    "        inferred_buildings_2020_parcel = inferred_buildings_2020_parcel.reset_index(drop=True)\n",
    "        inferred_buildings_2020_parcel = get_inference_main_building(\n",
    "                inferred_buildings_2020_parcel, osm_buildings_parcel)\n",
    "        inferred_buildings_2020_main_geom = inferred_buildings_2020_parcel.iloc[0]['geometry']\n",
    "\n",
    "    parcel_builds = inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, osm_buildings_parcel\n",
    "    parcel_main_geoms = inferred_buildings_2020_main_geom, inferred_buildings_2016_main_geom, osm_main_build_geom\n",
    "    \n",
    "    return parcel_builds, parcel_main_geoms\n",
    "\n",
    "\n",
    "\n",
    "def compute_largest_protruding_poly(union_build, base_build):\n",
    "    \"\"\"\n",
    "    Note we care about concentrated building\n",
    "    # expansions along a single wall and not general changes in building footprint due to noisy\n",
    "    # inferences so we isolate the largest protruding polygon\n",
    "    :return: (gpd.GeoSeries)\n",
    "    \"\"\"\n",
    "    diff_build = gpd.GeoDataFrame(geometry=[union_build.difference(base_build)], crs='EPSG:4326')\n",
    "\n",
    "    # Break up polygons\n",
    "    diff_build = diff_build.to_crs('EPSG:26910')\n",
    "    diff_build['geometry'] = diff_build.geometry.buffer(-0.2)\n",
    "    diff_build = diff_build.explode(ignore_index=True, index_parts=False)\n",
    "    diff_build['geometry'] = diff_build.geometry.buffer(0.2)\n",
    "    \n",
    "    # Remove \"pizza crusts\" or thin strips of additional area\n",
    "    diff_build['geometry'] = diff_build['geometry'].apply(\n",
    "    lambda geom: geom.buffer(-0.5).buffer(0.5*1.1).intersection(geom))\n",
    "    \n",
    "    diff_build = diff_build.to_crs('EPSG:4326')\n",
    "\n",
    "    # Return largest polygon\n",
    "    diff_build['area'] = diff_build.to_crs('EPSG:26910').area\n",
    "    diff_build = diff_build.sort_values('area', ascending=False).iloc[0]\n",
    "    \n",
    "    return diff_build\n",
    "  \n",
    "    \n",
    "def compare_building_footprint(base_geom, new_geom, diff_type, area_threshold):\n",
    "    expansion_flag, diff_value = None, None\n",
    "    if base_geom is not None:\n",
    "        expansion_flag = False\n",
    "\n",
    "        if diff_type == 'protruding_poly':\n",
    "            diff_gpd = compute_largest_protruding_poly(new_geom, base_geom)\n",
    "            diff_value = diff_gpd['area']\n",
    "        else:\n",
    "            raise Exception('[ERROR] Raw poly comparison not implemented.')\n",
    "\n",
    "        if diff_value > area_threshold:\n",
    "            expansion_flag = True\n",
    "    return expansion_flag, diff_value\n",
    "\n",
    "\n",
    "def flatten_geometries(gdf, threshold):\n",
    "    def check_overlapping_polygons(df, row):\n",
    "        unique = True\n",
    "        for i in set(range(len(df))).difference(set([row.name])):\n",
    "            intersect = ((row['geometry'].intersection(df.to_crs('EPSG:26910').iloc[i].geometry)).area) / row['geometry'].area\n",
    "            if intersect > threshold:\n",
    "                unique = False\n",
    "        return unique\n",
    "    \n",
    "    # Start by dropping duplicate geometries\n",
    "    gdf.drop_duplicates('geometry', inplace=True)\n",
    "    \n",
    "    gdf = gdf.copy()\n",
    "    gdf = gdf.reset_index(drop=True)\n",
    "    \n",
    "    gdf['unique'] = gdf.to_crs('EPSG:26910').apply(\n",
    "        lambda row: check_overlapping_polygons(gdf, row), axis=1)\n",
    "    \n",
    "    # Get unique geometries\n",
    "    gdf = gdf.loc[(gdf['unique'] == True) | ((gdf['main_building_flag'] == True) & (gdf['OSM_flag'] == False))]\n",
    "    \n",
    "    # Recompute area\n",
    "    gdf['area'] = gdf.to_crs('EPSG:26910').geometry.area\n",
    "    gdf = gdf.sort_values('area', ascending=False)\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecab5b1",
   "metadata": {},
   "source": [
    "## Plotting tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bea108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_osm_apn(parcel_apn, area_threshold, flatten_threshold):\n",
    "    inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, osm_buildings_parcel, permits_parcel = parcel_level_data(parcel_apn, sj_parcels_res, sj_parcel_permit)\n",
    "    parcel_bounds = sj_parcels_res[sj_parcels_res['APN'] == parcel_apn]\n",
    "\n",
    "    # Incorporate OSM data\n",
    "    parcel_buildings, _ = process_OSM_data(\n",
    "        inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, \n",
    "        osm_buildings_parcel, area_threshold, flatten_threshold, parcel_bounds)\n",
    "\n",
    "    # Plot\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(18, 8))\n",
    "    parcel_bounds.plot(ax=ax1, edgecolor='black', facecolor='none')\n",
    "    if osm_buildings_parcel is not None:\n",
    "        osm_buildings_parcel.plot(ax=ax1, color='blue', alpha=0.7)\n",
    "    if inferred_buildings_2020_parcel is not None:\n",
    "        inferred_buildings_2020_parcel.plot(ax=ax1, color='red', alpha=0.7)\n",
    "    ax1.axis('off')\n",
    "\n",
    "    parcel_bounds.plot(ax=ax2, edgecolor='black', facecolor='none')\n",
    "    if osm_buildings_parcel is not None:\n",
    "        osm_buildings_parcel.plot(ax=ax2, color='blue', alpha=0.7)\n",
    "    if inferred_buildings_2016_parcel is not None:\n",
    "        inferred_buildings_2016_parcel.plot(ax=ax2, color='red', alpha=0.7)\n",
    "    ax2.axis('off')\n",
    "\n",
    "    parcel_bounds.plot(ax=ax3, edgecolor='black', facecolor='none')\n",
    "    parcel_buildings.plot(ax=ax3, color='blue', alpha=0.7)\n",
    "    if osm_buildings_parcel is not None:\n",
    "        osm_buildings_parcel.plot(ax=ax3, color='blue', alpha=0)\n",
    "    ax3.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return parcel_buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f6159e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_osm_apn_full_detail(parcel_apn, area_threshold, flatten_threshold, sat_imagery=None, attach=None, title=True):\n",
    "    inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, osm_buildings_parcel, permits_parcel = parcel_level_data(parcel_apn, sj_parcels_res, sj_parcel_permit)\n",
    "    parcel_bounds = sj_parcels_res[sj_parcels_res['APN'] == parcel_apn]\n",
    "\n",
    "    # Incorporate OSM data\n",
    "    parcel_buildings, data_dict = process_OSM_data(\n",
    "        inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, \n",
    "        osm_buildings_parcel, area_threshold, flatten_threshold, parcel_bounds)\n",
    "\n",
    "    ib_2020_parcel = data_dict['2020_output']\n",
    "    ib_2016_parcel = data_dict['2016_output']\n",
    "    osm_parcel = data_dict['osm_output']\n",
    "    \n",
    "    # Define axis -- depends on whether we want an independent plot, or to attach to another plot\n",
    "    if attach is None:\n",
    "        if sat_imagery is None:\n",
    "            fig, axs = plt.subplots(ncols=3, nrows=2, figsize=(18, 10))\n",
    "        else:\n",
    "            fig, axs = plt.subplots(ncols=3, nrows=3, figsize=(18, 15))\n",
    "    else:\n",
    "        fig, axs = attach\n",
    "\n",
    "    # Get individual axis -- number depends on whether we visualize satellite imagery\n",
    "    if sat_imagery is None:\n",
    "        (ax1, ax2, ax3), (ax4, ax5, ax6) = axs\n",
    "    else:\n",
    "        (ax1, ax2, ax3), (ax4, ax5, ax6), (ax7, ax8, ax9) = axs\n",
    "\n",
    "    for ax in [a for alist in axs for a in alist]:\n",
    "        parcel_bounds.plot(ax=ax, edgecolor='black', facecolor='none')\n",
    "        ax.axis('off')\n",
    "        \n",
    "    for ax in (ax1, ax2):\n",
    "        if osm_buildings_parcel is not None:\n",
    "            osm_buildings_parcel.plot(ax=ax, color='blue', alpha=0.3)\n",
    "            \n",
    "    if inferred_buildings_2020_parcel is not None:\n",
    "        inferred_buildings_2020_parcel.plot(ax=ax1, color='red', alpha=0.7)\n",
    "    if title:\n",
    "        ax1.set_title('2020 inferences (raw)')\n",
    "        \n",
    "    if inferred_buildings_2016_parcel is not None:\n",
    "        inferred_buildings_2016_parcel.plot(ax=ax2, color='red', alpha=0.7)\n",
    "    if title:\n",
    "        ax2.set_title('2016 inferences (raw)')\n",
    "        \n",
    "    if osm_parcel is not None:\n",
    "        osm_parcel.plot(ax=ax3, color='blue', alpha=0.7)\n",
    "    if title:\n",
    "        ax3.set_title('OSM annotations')\n",
    "        \n",
    "    if ib_2020_parcel is not None:\n",
    "        ib_2020_parcel.plot(ax=ax4, color='red', alpha=0.7)\n",
    "    if title:\n",
    "        ax4.set_title('OSM-adjusted 2020 polygons')\n",
    "    \n",
    "    if ib_2016_parcel is not None:\n",
    "        ib_2016_parcel.plot(ax=ax5, color='red', alpha=0.7)\n",
    "    if title:\n",
    "        ax5.set_title('OSM-adjusted 2016 polygons')\n",
    "    \n",
    "    # Output\n",
    "    parcel_buildings.plot(ax=ax6, color='purple', alpha=0.7)\n",
    "    if title:\n",
    "        ax6.set_title('Output')\n",
    "    \n",
    "    # Satellite images\n",
    "    if sat_imagery is not None:\n",
    "        \n",
    "        for year, ax in zip(['2020', '2016'], (ax7, ax8)):\n",
    "            # Get imagery\n",
    "            file_name = get_file_name_from_parcel(\n",
    "                parcel_apn, sat_imagery['sj_parcels_res'], sat_imagery['tiles_gdf'][year])\n",
    "            img_file, superres_file = find_image_file_and_superrestile(\n",
    "                sat_imagery['img_fp'][year], sat_imagery['tif_fp'][year], file_name)\n",
    "\n",
    "            with rasterio.open(superres_file) as src:\n",
    "                out_image, out_transform = rasterio.mask.mask(\n",
    "                    src, parcel_bounds.to_crs('EPSG:26910')['geometry'], crop=True, nodata=255)\n",
    "            \n",
    "            # Plot\n",
    "            rasterio.plot.show(out_image, transform=out_transform, ax=ax)\n",
    "    \n",
    "    # Plot independently\n",
    "    if attach is None:\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    return parcel_buildings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb54f4d",
   "metadata": {},
   "source": [
    "## Visualizing satellite imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c94aff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name_from_parcel(parcel_apn, sj_parcels_res, tiles_gdf_year):\n",
    "    # Get parcel bounds\n",
    "    parcel_bounds = sj_parcels_res[sj_parcels_res['APN'] == parcel_apn]\n",
    "    \n",
    "    # Get tiles\n",
    "    tiles = tiles_gdf_year.copy()\n",
    "    \n",
    "    # Return tile with largest overlap with parcel\n",
    "    tiles['iou'] = tiles['geometry'].intersection(parcel_bounds.iloc[0]['geometry']).area\n",
    "    tiles = tiles.sort_values('iou', ascending=False)\n",
    "    tiles = tiles.iloc[0]\n",
    "    \n",
    "    return tiles['file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28439257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_image_file_and_superrestile(img_fp, tif_fp, file_name):\n",
    "    \"\"\"\n",
    "    Searches for the inference file within train, val and test directories.\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.join(img_fp, 'train')):\n",
    "        # For 2020 data which is split across train, val and test\n",
    "        img_file = None\n",
    "        for dirname in ['train', 'val', 'test']:\n",
    "            dirpath = os.path.join(img_fp, dirname, 'images', '{}.npy'.format(file_name))\n",
    "            if os.path.exists(dirpath):\n",
    "                img_file = dirpath\n",
    "    else:\n",
    "        # For 2016 and 2018 data which is not split\n",
    "        img_file = os.path.join(img_fp, '{}.npy'.format(file_name))\n",
    "        \n",
    "    # Generate file \n",
    "    superres_tile = os.path.join(img_fp, '..', 'superres_tif', '{}.tif'.format(file_name))\n",
    "    if not os.path.exists(superres_tile):\n",
    "        if not os.path.exists(os.path.join(img_fp, '..', 'superres_tif')):\n",
    "            os.makedirs(os.path.join(img_fp, '..', 'superres_tif'))\n",
    "        \n",
    "        tile_img = np.load(img_file).astype(np.uint8)\n",
    "        \n",
    "        # Get original raster\n",
    "        raster_original = rasterio.open(os.path.join(tif_fp, '{}.tif'.format(file_name)))\n",
    "        t = from_bounds(*raster_original.bounds, tile_img.shape[0], tile_img.shape[1])\n",
    "        raster_crs = rasterio.crs.CRS({\"init\": \"epsg:26910\"})\n",
    "        \n",
    "        with rasterio.open(superres_tile, 'w', driver='GTiff', \n",
    "                           height=tile_img.shape[0], width=tile_img.shape[1],\n",
    "                           count=3, dtype=str(tile_img.dtype),\n",
    "                           crs=raster_crs, transform=t) as raster_new:\n",
    "            raster_new.write(tile_img[:, :, 0], 1)\n",
    "            raster_new.write(tile_img[:, :, 1], 2)\n",
    "            raster_new.write(tile_img[:, :, 2], 3)\n",
    "            raster_new.close()\n",
    "        \n",
    "    return img_file, superres_tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c49829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tile_dicts_all_years(oak_fp, inferences_dir):\n",
    "    # Load tile dict for each year\n",
    "    tile_bounds_dict_all = {}\n",
    "    for year in ['2016', '2018', '2020']:\n",
    "        output_fp = os.path.join(oak_fp, 'outputs', 'cbg-inference-{}'.format(year))\n",
    "        with open(os.path.join(output_fp, 'tile_bounds.json'), \"r\") as f:\n",
    "            tile_bounds_dict = json.load(f)\n",
    "            tile_bounds_dict_all[year] = tile_bounds_dict\n",
    "    \n",
    "    # Get tiles for all years\n",
    "    tiles_gdf = {}\n",
    "    for year in ['2016', '2018', '2020']:\n",
    "        tiles = glob.glob(os.path.join(inferences_dir[year], '*.npy'))\n",
    "        tiles = [t.split(os.path.sep)[-1].replace('.npy', '') for t in tiles]\n",
    "        tile_metrics_pd = pd.DataFrame(tiles, columns=['file'])\n",
    "\n",
    "        tile_metrics_pd['geometry'] = tile_metrics_pd.file.progress_apply(\n",
    "            lambda name: get_bounds(tile_bounds_dict_all[year], name) if name in list(tile_bounds_dict_all[year].keys()) else None\n",
    "        )\n",
    "        tiles_gdf[year] = gpd.GeoDataFrame(tile_metrics_pd.copy(), crs='EPSG:4326')\n",
    "        \n",
    "    return tile_bounds_dict_all, tiles_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4015827a",
   "metadata": {},
   "source": [
    "## Ground truth cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e614bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ground_truth_parcels():\n",
    "    # Positive small building constructions\n",
    "    ps_gt_grid = (['23044043', '24960042', '68932067', '27447043', '69414018'], \n",
    "                  'Positive small build')\n",
    "\n",
    "    # Negative small building constructions\n",
    "    ns_gt_grid = (['24960056', '27406055', '42937040', '47701057', '44249007', '40306200', \n",
    "                   '09218018', '46742046', '27725060', '26434063'], \n",
    "                  'Negative small build')\n",
    "\n",
    "    # Positive main building constructions\n",
    "    pm_gt_grid = (['58630050', '42905080', '47202096', '24960042', '48608012', '41933001'], \n",
    "                  'Positive main build')\n",
    "\n",
    "    # Negative main building constructions\n",
    "    nm_gt_grid = (['41934035', '49936015', '49722020', '37804025', '44710075', '44234038', \n",
    "                   '46702025', '43944074', '42116035', '24509050', '26444013', '70845021', \n",
    "                   '48809009'],\n",
    "                  'Negative main build')\n",
    "    case_dict = {\n",
    "        'Positive small build': ps_gt_grid,\n",
    "        'Negative small build': ns_gt_grid,\n",
    "        'Positive main build': pm_gt_grid,\n",
    "        'Negative main build': nm_gt_grid\n",
    "    }\n",
    "    return case_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
