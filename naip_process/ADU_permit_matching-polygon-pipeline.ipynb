{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12a61d4b",
   "metadata": {},
   "source": [
    "# Functions to run the polygon pipeline for ADU permit-matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be8dc8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import rasterio\n",
    "from rasterio.transform import from_bounds\n",
    "from rasterio import plot\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fada068",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec01611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sources():\n",
    "    tif_fp = {\n",
    "    '2016': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/2016/raw_tif', \n",
    "    '2018': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/2018/raw_tif',\n",
    "    '2020': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/2020/raw_tif'\n",
    "    }\n",
    "    inferences_dir = {\n",
    "        '2016': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/2016/infer',\n",
    "        '2018': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/2018/infer',\n",
    "        '2020': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/2020/infer'\n",
    "    }\n",
    "    img_fp = {\n",
    "        '2016': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/2016/superresx2',\n",
    "        '2018': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/2018/superresx2',\n",
    "        #'2020': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/phase2_superresx2'\n",
    "        '2020': '/oak/stanford/groups/deho/building_compliance/san_jose_naip_512/2020/superresx2'\n",
    "    }\n",
    "    return tif_fp, inferences_dir, img_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9520c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parcel_level_data(parcel_apn, sj_parcels_res, sj_parcel_permit):\n",
    "    parcel_bounds = sj_parcels_res[sj_parcels_res['APN'] == parcel_apn]\n",
    "    \n",
    "    # Permits\n",
    "    permits_parcel = sj_parcel_permit[sj_parcel_permit['APN_parcel'] == parcel_apn]\n",
    "    if permits_parcel.empty:\n",
    "        permits_parcels = None\n",
    "\n",
    "    def mask_buildings1(parcel_bounds, fp):\n",
    "        df_out = gpd.read_file(fp, mask=parcel_bounds)\n",
    "        df_out['iou'] = df_out['geometry'].intersection(parcel_bounds).area/df_out['geometry'].area\n",
    "        df_out = df_out[df_out['iou'] > 0.7]\n",
    "        if df_out.empty:\n",
    "            return None\n",
    "        else:\n",
    "            return df_out\n",
    "        \n",
    "    if len(parcel_bounds['geometry']) == 0:\n",
    "        return None, None, None, permits_parcels\n",
    "\n",
    "    inferred_buildings_2020_parcel = mask_buildings1(parcel_bounds['geometry'].values[0], BUILD_FP.format('2020'))\n",
    "    inferred_buildings_2016_parcel = mask_buildings1(parcel_bounds['geometry'].values[0], BUILD_FP.format('2016'))\n",
    "    osm_buildings_parcel = mask_buildings1(parcel_bounds['geometry'].values[0], OSM_FP)\n",
    "\n",
    "    return inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, osm_buildings_parcel, permits_parcel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6aaec7",
   "metadata": {},
   "source": [
    "## Process polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea5798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_OSM_data(inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, \n",
    "                     osm_buildings_parcel, parcel_bounds, model_params):\n",
    "    \"\"\"\n",
    "    Used for the permit-matching pipeline to flag small and main building construction/expansions\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Returns a gpd.GeoDataFrame with the following columns:\n",
    "    #   - GEOID, area, small, large\n",
    "    #   - flags: OSM_flag, expansion_OSM_flag, main_building_flag, expansion_2016_flag\n",
    "    #   - building geometry\n",
    "    \n",
    "    # Check model params\n",
    "    param_keys = ['area_threshold_main', 'area_threshold_small', \n",
    "                  'main_expansion_type', 'main_polygon_definition',\n",
    "                  'negative_buffer', 'simplify_tolerance', 'flatten_threshold']\n",
    "    for param in param_keys:\n",
    "        assert param in list(model_params.keys())\n",
    "    assert model_params['main_expansion_type'] in ['raw_polygons', 'MRR', 'convex_hull', \n",
    "                                                   'osm_union', 'raw_polygons_iou']\n",
    "    assert model_params['main_polygon_definition'] in ['Largest', 'OSM']\n",
    "    \n",
    "    gpd_cols = [\n",
    "        'main_building_flag', 'OSM_flag', 'expansion_OSM_flag', 'diff_OSM_value',\n",
    "        'expansion_2016_flag', 'diff_2016_value', 'geometry']\n",
    "    parcel_buildings = gpd.GeoDataFrame(geometry=[], columns=gpd_cols)\n",
    "    \n",
    "    # Drop OSM index_left column\n",
    "    if osm_buildings_parcel is not None and 'index_left' in osm_buildings_parcel.columns:\n",
    "        osm_buildings_parcel.drop('index_left', axis=1, inplace=True)\n",
    "        \n",
    "    # Identify main buildings\n",
    "    parcel_builds, parcel_main_geoms = identify_main_buildings(\n",
    "        inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, \n",
    "        osm_buildings_parcel, parcel_bounds, model_params)\n",
    "    ib_2020_parcel, ib_2016_parcel, osm_parcel = parcel_builds\n",
    "    ib_2020_main_geom, ib_2016_main_geom, osm_main_geom = parcel_main_geoms\n",
    "    \n",
    "    # Process the 2016 inferences\n",
    "    # Note: We process 2016 in a special manner. We do not default to OSM as we do for\n",
    "    # 2020, but rather just use OSM to complete the predictions that were inferred\n",
    "    # by the model. \n",
    "    \n",
    "    # Merge buildings with OSM annotations\n",
    "    ib_2016_parcel = merge_buildings(\n",
    "        gdf=ib_2016_parcel, comp=osm_parcel, model_params=model_params, limit_to_inferences=True)\n",
    "    \n",
    "    # Case 1: No inference nor OSM data ----------------- \n",
    "    if ib_2020_parcel is None and osm_parcel is None:\n",
    "        return parcel_buildings[gpd_cols], np.NAN\n",
    "\n",
    "    \n",
    "    # Case 2: No inference but OSM data ----------------- \n",
    "    # We fully rely on OSM data\n",
    "    if ib_2020_parcel is None and osm_parcel is not None:\n",
    "        parcel_buildings = osm_parcel.copy()\n",
    "        parcel_buildings['OSM_flag'] = True\n",
    "        \n",
    "        # Compare to 2016 footprints\n",
    "        parcel_buildings = compare_buildings(\n",
    "            gdf=parcel_buildings, comp_list=[ib_2016_parcel], name_list=['2016'], \n",
    "            model_params=model_params)\n",
    "        parcel_buildings['expansion_OSM_flag'] = False\n",
    "        parcel_buildings['diff_OSM_value'] = 0\n",
    "        \n",
    "        \n",
    "    # Case 3: Inference and no OSM data ----------------- \n",
    "    if osm_parcel is None and ib_2020_parcel is not None:\n",
    "        \n",
    "        # Compare to 2016 footprints\n",
    "        parcel_buildings = compare_buildings(\n",
    "            gdf=ib_2020_parcel, comp_list=[ib_2016_parcel], name_list=['2016'],  \n",
    "            model_params=model_params)\n",
    "        \n",
    "        # Reflect lack of OSM\n",
    "        parcel_buildings['OSM_flag'] = False\n",
    "        parcel_buildings['expansion_OSM_flag'] = None\n",
    "        parcel_buildings['diff_OSM_value'] = None\n",
    "\n",
    "        \n",
    "    # Case 4: Inference and OSM data -----------------   \n",
    "    if osm_parcel is not None and ib_2020_parcel is not None:\n",
    "        # Merge with OSM footprints\n",
    "        ib_2020_parcel = merge_buildings(\n",
    "            gdf=ib_2020_parcel, comp=osm_parcel, model_params=model_params, limit_to_inferences=False)\n",
    "        \n",
    "        # Check for 2016 and OSM expansions\n",
    "        parcel_buildings = compare_buildings(\n",
    "            gdf=ib_2020_parcel, comp_list=[osm_parcel, ib_2016_parcel], name_list=['OSM', '2016'],  \n",
    "            model_params=model_params\n",
    "            )\n",
    "\n",
    "    # Raw main building expansion check for 2020 vs 2016\n",
    "    if model_params['main_expansion_type'] == 'raw_polygons_iou':\n",
    "        if ib_2020_parcel is not None and ib_2016_parcel is not None:\n",
    "            raw_main_2020 = ib_2020_main_geom\n",
    "            raw_main_2016 = ib_2016_main_geom\n",
    "            main_exp = (raw_main_2020.intersection(raw_main_2016)).area / (raw_main_2020.union(raw_main_2016)).area\n",
    "            if main_exp < 0.8:\n",
    "                parcel_buildings.loc[(parcel_buildings['main_building_flag'] == True), 'expansion_2016_flag'] = True\n",
    "\n",
    "    # Compute building area\n",
    "    parcel_buildings['area'] = parcel_buildings.to_crs('EPSG:26910').geometry.area\n",
    "    \n",
    "    # Generate data dict (for debugging)\n",
    "    data_dict = {'2020_output': ib_2020_parcel, '2016_output': ib_2016_parcel, 'osm_output': osm_parcel}\n",
    "    return parcel_buildings[gpd_cols + ['area']], data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0016bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_buildings(gdf, comp_list, name_list, model_params):\n",
    "    \"\"\"\n",
    "    Compares the area of each building in gdf to the buildings in the GeoDataFrames in\n",
    "    comp_list to check for expansions/constructions.\n",
    "    \"\"\"\n",
    "    match_cols = ['GEOID', 'area', 'iou', 'main_building_flag',\n",
    "                  'OSM_flag', 'geometry'] +  [\n",
    "        'expansion_{}_flag'.format(comp_name) for comp_name in name_list] + [\n",
    "        'diff_{}_value'.format(comp_name) for comp_name in name_list]\n",
    "    \n",
    "    if gdf is None:\n",
    "        return None\n",
    "    \n",
    "    for comp_name in name_list:\n",
    "        gdf['expansion_{}_flag'.format(comp_name)] = None\n",
    "        gdf['diff_{}_value'.format(comp_name)] = None\n",
    "    \n",
    "    for comp, comp_name in zip(comp_list, name_list):\n",
    "        \n",
    "        if comp is None:\n",
    "            continue\n",
    "        \n",
    "        # Check expansion\n",
    "        if len(gdf) > 0:\n",
    "            comp['geometry_comp'] = comp['geometry']\n",
    "\n",
    "            # Get polygon in comp gdf to which we compare each polygon\n",
    "            comp = comp.reset_index()\n",
    "            gdf = gdf.sjoin(comp[['geometry', 'geometry_comp']], how='left', predicate='intersects')\n",
    "            gdf['iou_comp'] = gdf.apply(lambda row: (\n",
    "                row['geometry'].intersection(comp.iloc[int(row['index_right'])]['geometry'])).area / \n",
    "                                    row['geometry'].area if pd.notnull(row['index_right']) else None, axis=1)\n",
    "\n",
    "            # Have to account for potentially various matches for one inference\n",
    "            gdf = gdf.sort_values('iou_comp', ascending=False)\n",
    "            gdf.drop_duplicates(subset=['geometry'], keep='first', inplace=True)\n",
    "\n",
    "            # Check expansion for each polygon\n",
    "            gdf[['expansion_{}_flag'.format(comp_name), 'diff_{}_value'.format(comp_name)]] = gdf.apply(\n",
    "                lambda row: compare_building_footprint(\n",
    "                    base_geom=comp.iloc[int(row['index_right'])]['geometry'], \n",
    "                    new_geom=row['geometry'].union(comp.iloc[int(row['index_right'])]['geometry']), \n",
    "                    diff_type='protruding_poly', \n",
    "                    model_params=model_params, \n",
    "                    main_building_flag=row['main_building_flag']\n",
    "                ) if pd.notnull(row['index_right']) else (True, True), \n",
    "                axis=1, result_type=\"expand\")\n",
    "            \n",
    "            # Get area difference for each polygon\n",
    "            gdf.loc[gdf['diff_{}_value'.format(comp_name)] == True, 'diff_{}_value'.format(comp_name)] = gdf.loc[\n",
    "                gdf['diff_{}_value'.format(comp_name)] == True].to_crs('EPSG:26910')['geometry'].area\n",
    "\n",
    "        gdf = gdf[match_cols]\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7332ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_buildings(gdf, comp, model_params, limit_to_inferences):\n",
    "    match_cols = ['GEOID', 'area', 'iou', 'main_building_flag', 'OSM_flag', 'geometry'] \n",
    "    \n",
    "    if gdf is None:\n",
    "        return None\n",
    "    \n",
    "    if comp is None:\n",
    "        return gdf\n",
    "    \n",
    "    # Default to OSM for all buildings except main building\n",
    "    parcel_build = comp.loc[comp['main_building_flag'] == False].copy()\n",
    "    \n",
    "    # Check for missing buildings in OSM annotations\n",
    "    parcel_union = parcel_build.geometry.unary_union\n",
    "    \n",
    "    fp = gdf.loc[gdf['main_building_flag'] == False].copy()\n",
    "    if len(fp) > 0:\n",
    "        fp['inf_not_covered'] = fp.apply(\n",
    "            lambda row: compare_building_footprint(\n",
    "                base_geom=parcel_union, \n",
    "                new_geom=row['geometry'], \n",
    "                diff_type='protruding_poly', \n",
    "                model_params=model_params, \n",
    "                main_building_flag=row['main_building_flag'])[0],\n",
    "            axis=1)\n",
    "\n",
    "        fp = fp.sjoin(parcel_build[['geometry']], how='left')\n",
    "        fp = fp.loc[(fp['inf_not_covered'] == True) | (fp['index_right'].isna())]\n",
    "\n",
    "        # Account for multiple OSM matches\n",
    "        fp.drop_duplicates(subset=['geometry'], inplace=True)\n",
    "    fp['OSM_flag'] = False\n",
    "    \n",
    "    # Keep only buildings identified in inferences (we're lenient and allow for anything)\n",
    "    # that is at least 30% covered by the inferences to be included or \n",
    "    # inference footprint is 60% covered by OSM.\n",
    "\n",
    "    if limit_to_inferences and len(parcel_build) > 0:\n",
    "        # Cover 30% of OSM footprints\n",
    "        gdf_geom = gdf.geometry.unary_union\n",
    "        parcel_build['osm_coverage'] = parcel_build['geometry'].intersection(gdf_geom).area/parcel_build['geometry'].area\n",
    "        \n",
    "        # OSM covers 60% of inference footprint\n",
    "        gdf.reset_index(inplace=True, drop=True)\n",
    "        parcel_build = parcel_build.sjoin(gdf[['geometry']], how='left', predicate='intersects')\n",
    "        \n",
    "        parcel_build['inf_coverage'] = parcel_build.apply(\n",
    "            lambda row: 0 if pd.isnull(row['index_right']) else row['geometry'].intersection(\n",
    "                gdf.iloc[int(row['index_right'])]['geometry']).area/gdf.iloc[\n",
    "                int(row['index_right'])]['geometry'].area, axis=1)\n",
    "        \n",
    "        parcel_build = parcel_build.loc[(parcel_build['osm_coverage'] > 0.3) | (parcel_build['inf_coverage'] > 0.6)]\n",
    "        \n",
    "    # Concatenate OSM small buildings and small buildings missed by OSM\n",
    "    parcel_build = pd.concat([parcel_build[match_cols], fp[match_cols]])\n",
    "    \n",
    "    # Add back main building\n",
    "    parcel_build = pd.concat([gdf.loc[gdf['main_building_flag'] == True][match_cols], \n",
    "                              parcel_build[match_cols]])\n",
    "    \n",
    "    # Flatten\n",
    "    gdf = flatten_geometries(gdf=parcel_build, model_params=model_params)\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef9d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_main_buildings(inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, \n",
    "                            osm_buildings_parcel, parcel_bounds, model_params):\n",
    "    # Identify OSM main building\n",
    "    osm_main_build_geom, osm_main_build = None, None\n",
    "    \n",
    "    if osm_buildings_parcel is not None:\n",
    "        # Filter duplicate geometries\n",
    "        osm_buildings_parcel.drop_duplicates(subset=['geometry'], inplace=True)\n",
    "        \n",
    "        osm_buildings_parcel = osm_buildings_parcel.sort_values('area', ascending=False)\n",
    "        \n",
    "        # Identify main building\n",
    "        osm_buildings_parcel = osm_buildings_parcel.reset_index(drop=True)\n",
    "        osm_buildings_parcel['main_building_flag'] = osm_buildings_parcel.apply(\n",
    "            lambda row: True if row.name == 0 else False, axis=1)\n",
    "        \n",
    "        osm_buildings_parcel['OSM_flag'] = True\n",
    "        osm_main_build_geom = osm_buildings_parcel.iloc[0]['geometry']\n",
    "        \n",
    "    # Identify main 2016 building\n",
    "    inferred_buildings_2016_main_geom = None\n",
    "    if inferred_buildings_2016_parcel is not None:\n",
    "        # Clip inferences\n",
    "        inferred_buildings_2016_parcel = gpd.clip(inferred_buildings_2016_parcel, parcel_bounds)\n",
    "        \n",
    "        inferred_buildings_2016_parcel = inferred_buildings_2016_parcel.reset_index(drop=True)\n",
    "        inferred_buildings_2016_parcel = get_inference_main_building(\n",
    "            inferred_buildings_2016_parcel, osm_buildings_parcel, model_params)\n",
    "        \n",
    "        inferred_buildings_2016_main_geom = inferred_buildings_2016_parcel.iloc[0]['geometry'] \n",
    "    \n",
    "    # Identify main 2020 building\n",
    "    inferred_buildings_2020_main_geom = None\n",
    "    if inferred_buildings_2020_parcel is not None:\n",
    "        # Clip inferences\n",
    "        inferred_buildings_2020_parcel = gpd.clip(inferred_buildings_2020_parcel, parcel_bounds)\n",
    "        \n",
    "        inferred_buildings_2020_parcel = inferred_buildings_2020_parcel.reset_index(drop=True)\n",
    "        inferred_buildings_2020_parcel = get_inference_main_building(\n",
    "                inferred_buildings_2020_parcel, osm_buildings_parcel, model_params)\n",
    "        inferred_buildings_2020_main_geom = inferred_buildings_2020_parcel.iloc[0]['geometry']\n",
    "\n",
    "    parcel_builds = inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, osm_buildings_parcel\n",
    "    parcel_main_geoms = inferred_buildings_2020_main_geom, inferred_buildings_2016_main_geom, osm_main_build_geom\n",
    "    \n",
    "    return parcel_builds, parcel_main_geoms\n",
    "\n",
    "\n",
    "def define_main_building(inf_main_build, osm_main_build, model_params):\n",
    "    \n",
    "    if model_params['main_expansion_type'] == 'MRR':\n",
    "        # Replace with OSM coverage (as determined by the minimum rotated rectangle) plus\n",
    "        # whatever was not covered in OSM (potential expansions)\n",
    "        inf_mrr = inf_main_build.minimum_rotated_rectangle\n",
    "        inf_union = inf_mrr.intersection(osm_main_build.geometry.unary_union)\n",
    "        inf_union = inf_union.union(inf_main_build)\n",
    "        inf_main_build = inf_union\n",
    "\n",
    "    elif model_params['main_expansion_type'] == 'convex_hull':\n",
    "        inf_ch = inf_main_build.convex_hull\n",
    "        inf_union = inf_ch.intersection(osm_main_build.geometry.unary_union)\n",
    "        inf_union = inf_union.union(inf_main_build)\n",
    "        inf_main_build = inf_union\n",
    "\n",
    "    elif model_params['main_expansion_type'] in ['raw_polygons', 'raw_polygons_iou']:\n",
    "        # We keep the raw polygons that overlap with the OSM building\n",
    "        pass\n",
    "    \n",
    "    elif model_params['main_expansion_type'] == 'osm_union':\n",
    "        # Replace with union of overlapping poly and OSM\n",
    "        inf_union = inf_main_build.union(osm_main_build.geometry.unary_union)\n",
    "        inf_main_build = inf_union\n",
    "        \n",
    "    else:\n",
    "        raise Exception('[ERROR] Invalid main_expansion_type')\n",
    "        \n",
    "    return inf_main_build\n",
    "\n",
    "\n",
    "def get_inference_main_building(inference_buildings, osm_buildings_parcel, model_params):\n",
    "    \"\"\"\n",
    "    inference_buildings: (gpd.GeoDataFrame) parcel inference all buildings\n",
    "    osm_buildings_parcel: (gpd.GeoDataFrame) parcel OSM all buildings\n",
    "    \"\"\"\n",
    "    \n",
    "    if inference_buildings is None:\n",
    "        return None\n",
    "    \n",
    "    inference_buildings = inference_buildings.sort_values(\n",
    "            'area', ascending=False)\n",
    "    cols = ['GEOID', 'area',  'geometry', 'iou', 'OSM_flag', 'main_building_flag']\n",
    "    \n",
    "    # If OSM is unavailable, we extract the largest polygon\n",
    "    if osm_buildings_parcel is None or model_params['main_polygon_definition'] == 'Largest':\n",
    "        inference_buildings = inference_buildings.reset_index(drop=True)\n",
    "        inference_buildings['main_building_flag'] = inference_buildings.apply(\n",
    "            lambda row: True if row.name == 0 else False, axis=1)\n",
    "        inference_buildings['OSM_flag'] = False\n",
    "        \n",
    "        # Simplify\n",
    "        inference_buildings = simplify_gdf(inference_buildings, model_params)\n",
    "        \n",
    "    # If OSM is available, we use the union between the polygons that overlap with \n",
    "    # the main building in OSM and the main building in OSM.\n",
    "    else:\n",
    "        # OSM main build\n",
    "        osm_main_build = osm_buildings_parcel.loc[osm_buildings_parcel['main_building_flag'] == True]\n",
    "        \n",
    "        # Identify inference buildings overlapping with OSM main building\n",
    "        inference_buildings = inference_buildings.sjoin(\n",
    "            osm_main_build[['geometry']], how='left', predicate='intersects')\n",
    "        inference_buildings['OSM_flag'] = False\n",
    "        \n",
    "        # If there is no overlap with the OSM main building, we use OSM.\n",
    "        if inference_buildings['index_right'].isna().mean() == 1:\n",
    "            inference_buildings['main_building_flag'] = False\n",
    "            \n",
    "            # Simplify small builds\n",
    "            inference_buildings = simplify_gdf(inference_buildings, model_params)\n",
    "            \n",
    "            inference_buildings = pd.concat([osm_main_build[cols], inference_buildings[cols]])\n",
    "            inference_buildings = inference_buildings.sort_values('area', ascending=False)\n",
    "        else:\n",
    "            # Identify the inference blobs associated with the main OSM building\n",
    "            inference_buildings['main_building_flag'] = inference_buildings['index_right'].apply(\n",
    "                lambda x: True if pd.notnull(x) else False)\n",
    "            \n",
    "            # Combine main building polygon\n",
    "            inference_buildings['dissolve_idx'] = np.arange(len(inference_buildings))\n",
    "            inference_buildings['dissolve_idx'] = inference_buildings.apply(lambda row: 99 if row['main_building_flag'] else row['dissolve_idx'], axis=1)\n",
    "            inference_buildings = inference_buildings.dissolve(\n",
    "                by='dissolve_idx', aggfunc={\n",
    "                     \"area\": \"sum\",\n",
    "                     'GEOID': 'first',\n",
    "                     'iou': 'mean',\n",
    "                     'main_building_flag': 'max'\n",
    "                 },).reset_index()\n",
    "            inference_buildings.drop(['dissolve_idx'], axis=1, inplace=True)\n",
    "            inference_buildings = inference_buildings.sort_values('area', ascending=False)\n",
    "            inference_buildings = inference_buildings.reset_index(drop=True)\n",
    "            \n",
    "            # Simplify\n",
    "            inference_buildings = simplify_gdf(inference_buildings, model_params)\n",
    "            inference_buildings['OSM_flag'] = False\n",
    "            \n",
    "            # * Choose our definition of main inference building * \n",
    "            inf_main_build = inference_buildings.iloc[0]['geometry']\n",
    "            inf_main_build = define_main_building(inf_main_build, osm_main_build, model_params)\n",
    "            \n",
    "            # * Remove overlap with OSM small buildings\n",
    "            osm_small_build = osm_buildings_parcel.loc[osm_buildings_parcel['main_building_flag'] == False]\n",
    "            if len(osm_small_build) > 0:\n",
    "                inf_main_build_diff = inf_main_build.difference(osm_small_build.geometry.unary_union)\n",
    "                \n",
    "                # Add back the OSM small buildings we removed from main inference\n",
    "                if inf_main_build != inf_main_build_diff:\n",
    "                    covered_osm_buildings = osm_small_build.sjoin(gpd.GeoDataFrame(geometry=[inf_main_build]))\n",
    "                    covered_osm_buildings = covered_osm_buildings.loc[~covered_osm_buildings['index_right'].isna()]\n",
    "                    # * Keep only the portions actually captured by the main building inference\n",
    "                    covered_osm_buildings['geometry'] = covered_osm_buildings['geometry'].apply(\n",
    "                        lambda geom: geom.intersection(inf_main_build))\n",
    "                    covered_osm_buildings['OSM_flag'] = False\n",
    "                    inference_buildings = pd.concat([inference_buildings[cols], covered_osm_buildings[cols]])\n",
    "                \n",
    "                # * Keep largest polygon (in case OSM difference broke up the polygon and main building\n",
    "                # was not a multipolygon prior to this operation)\n",
    "                shp_mpoly = shapely.geometry.multipolygon.MultiPolygon\n",
    "                if type(inf_main_build_diff) == shp_mpoly and type(inf_main_build) != shp_mpoly:\n",
    "                    inf_main_build_diff = max(inf_main_build_diff, key=lambda a: a.area)\n",
    "                    \n",
    "                inf_main_build = inf_main_build_diff\n",
    "            \n",
    "            inference_buildings.at[0, 'geometry'] = inf_main_build\n",
    "\n",
    "    return inference_buildings[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f28400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_gdf(gdf, model_params):\n",
    "    # Get params\n",
    "    simplify_tolerance = model_params['simplify_tolerance']\n",
    "    \n",
    "    gdf = gdf.to_crs('EPSG:26910')\n",
    "    gdf['geometry'] = gdf['geometry'].simplify(tolerance=simplify_tolerance, preserve_topology=True)\n",
    "    gdf = gdf.to_crs('EPSG:4326')\n",
    "    return gdf\n",
    "\n",
    "def compute_largest_protruding_poly(union_build, base_build, model_params):\n",
    "    \"\"\"\n",
    "    Note we care about concentrated building\n",
    "    # expansions along a single wall and not general changes in building footprint due to noisy\n",
    "    # inferences so we isolate the largest protruding polygon\n",
    "    :return: (gpd.GeoSeries)\n",
    "    \"\"\"\n",
    "    # Get params\n",
    "    negative_buffer = model_params['negative_buffer']\n",
    "    \n",
    "    diff_build = gpd.GeoDataFrame(geometry=[union_build.difference(base_build)], crs='EPSG:4326')\n",
    "\n",
    "    # Break up polygons\n",
    "    diff_build = diff_build.to_crs('EPSG:26910')\n",
    "    diff_build['geometry'] = diff_build.geometry.buffer(-0.2)\n",
    "    diff_build = diff_build.explode(ignore_index=True, index_parts=False)\n",
    "    diff_build['geometry'] = diff_build.geometry.buffer(0.2)\n",
    "    \n",
    "    # Remove \"pizza crusts\" or thin strips of additional area\n",
    "    diff_build['geometry'] = diff_build['geometry'].apply(\n",
    "    lambda geom: geom.buffer(-negative_buffer).buffer(negative_buffer*1.1).intersection(geom))\n",
    "    \n",
    "    diff_build = diff_build.to_crs('EPSG:4326')\n",
    "\n",
    "    # Return largest polygon\n",
    "    diff_build['area'] = diff_build.to_crs('EPSG:26910').area\n",
    "    diff_build = diff_build.sort_values('area', ascending=False).iloc[0]\n",
    "    \n",
    "    return diff_build\n",
    "  \n",
    "    \n",
    "def compare_building_footprint(base_geom, new_geom, diff_type, model_params, main_building_flag):\n",
    "    \n",
    "    # Get params\n",
    "    area_threshold_main = model_params['area_threshold_main']\n",
    "    area_threshold_small = model_params['area_threshold_small']\n",
    "    \n",
    "    if main_building_flag:\n",
    "        area_threshold = area_threshold_main\n",
    "    else:\n",
    "        area_threshold = area_threshold_small\n",
    "    \n",
    "    expansion_flag, diff_value = None, None\n",
    "    if base_geom is not None:\n",
    "        expansion_flag = False\n",
    "\n",
    "        if diff_type == 'protruding_poly':\n",
    "            diff_gpd = compute_largest_protruding_poly(new_geom, base_geom, model_params)\n",
    "            diff_value = diff_gpd['area']\n",
    "        else:\n",
    "            raise Exception('[ERROR] Raw poly comparison not implemented.')\n",
    "\n",
    "        if diff_value > area_threshold:\n",
    "            expansion_flag = True\n",
    "    return expansion_flag, diff_value\n",
    "\n",
    "\n",
    "def flatten_geometries(gdf, model_params):\n",
    "    threshold = model_params['flatten_threshold']\n",
    "    \n",
    "    def check_overlapping_polygons(df, row):\n",
    "        if row['geometry'].area == 0:\n",
    "            return False\n",
    "        \n",
    "        unique = True\n",
    "        for i in set(range(len(df))).difference(set([row.name])):\n",
    "            intersect = ((row['geometry'].intersection(df.to_crs('EPSG:26910').iloc[i].geometry)).area) / row['geometry'].area\n",
    "            if intersect > threshold:\n",
    "                unique = False\n",
    "        return unique\n",
    "    \n",
    "    # Start by dropping duplicate geometries\n",
    "    gdf.drop_duplicates('geometry', inplace=True)\n",
    "    \n",
    "    gdf = gdf.copy()\n",
    "    gdf = gdf.reset_index(drop=True)\n",
    "    \n",
    "    gdf['unique'] = gdf.to_crs('EPSG:26910').apply(\n",
    "        lambda row: check_overlapping_polygons(gdf, row), axis=1)\n",
    "    \n",
    "    # Get unique geometries\n",
    "    gdf = gdf.loc[(gdf['unique'] == True) | ((gdf['main_building_flag'] == True) & (gdf['OSM_flag'] == False))]\n",
    "    \n",
    "    # Recompute area\n",
    "    gdf['area'] = gdf.to_crs('EPSG:26910').geometry.area\n",
    "    gdf = gdf.sort_values('area', ascending=False)\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecab5b1",
   "metadata": {},
   "source": [
    "## Plotting tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bea108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_osm_apn(parcel_apn, model_params):\n",
    "    inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, osm_buildings_parcel, permits_parcel = parcel_level_data(parcel_apn, sj_parcels_res, sj_parcel_permit)\n",
    "    parcel_bounds = sj_parcels_res[sj_parcels_res['APN'] == parcel_apn]\n",
    "\n",
    "    # Incorporate OSM data\n",
    "    parcel_buildings, _ = process_OSM_data(\n",
    "        inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, \n",
    "        osm_buildings_parcel, parcel_bounds, model_params)\n",
    "\n",
    "    # Plot\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(18, 8))\n",
    "    parcel_bounds.plot(ax=ax1, edgecolor='black', facecolor='none')\n",
    "    if osm_buildings_parcel is not None:\n",
    "        osm_buildings_parcel.plot(ax=ax1, color='blue', alpha=0.7)\n",
    "    if inferred_buildings_2020_parcel is not None:\n",
    "        inferred_buildings_2020_parcel.plot(ax=ax1, color='red', alpha=0.7)\n",
    "    ax1.axis('off')\n",
    "\n",
    "    parcel_bounds.plot(ax=ax2, edgecolor='black', facecolor='none')\n",
    "    if osm_buildings_parcel is not None:\n",
    "        osm_buildings_parcel.plot(ax=ax2, color='blue', alpha=0.7)\n",
    "    if inferred_buildings_2016_parcel is not None:\n",
    "        inferred_buildings_2016_parcel.plot(ax=ax2, color='red', alpha=0.7)\n",
    "    ax2.axis('off')\n",
    "\n",
    "    parcel_bounds.plot(ax=ax3, edgecolor='black', facecolor='none')\n",
    "    parcel_buildings.plot(ax=ax3, color='blue', alpha=0.7)\n",
    "    if osm_buildings_parcel is not None:\n",
    "        osm_buildings_parcel.plot(ax=ax3, color='blue', alpha=0)\n",
    "    ax3.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return parcel_buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f6159e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_osm_apn_full_detail(parcel_apn, model_params, sat_imagery=None, attach=None, title=True):\n",
    "    inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, osm_buildings_parcel, permits_parcel = parcel_level_data(parcel_apn, sj_parcels_res, sj_parcel_permit)\n",
    "    parcel_bounds = sj_parcels_res[sj_parcels_res['APN'] == parcel_apn]\n",
    "\n",
    "    # Incorporate OSM data\n",
    "    parcel_buildings, data_dict = process_OSM_data(\n",
    "        inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, \n",
    "        osm_buildings_parcel, parcel_bounds, model_params)\n",
    "\n",
    "    ib_2020_parcel = data_dict['2020_output']\n",
    "    ib_2016_parcel = data_dict['2016_output']\n",
    "    osm_parcel = data_dict['osm_output']\n",
    "    \n",
    "    # Define axis -- depends on whether we want an independent plot, or to attach to another plot\n",
    "    if attach is None:\n",
    "        if sat_imagery is None:\n",
    "            fig, axs = plt.subplots(ncols=3, nrows=2, figsize=(18, 10))\n",
    "        else:\n",
    "            fig, axs = plt.subplots(ncols=3, nrows=3, figsize=(18, 15))\n",
    "    else:\n",
    "        fig, axs = attach\n",
    "\n",
    "    # Get individual axis -- number depends on whether we visualize satellite imagery\n",
    "    if sat_imagery is None:\n",
    "        (ax1, ax2, ax3), (ax4, ax5, ax6) = axs\n",
    "    else:\n",
    "        (ax1, ax2, ax3), (ax4, ax5, ax6), (ax7, ax8, ax9) = axs\n",
    "\n",
    "    for ax in [a for alist in axs for a in alist]:\n",
    "        parcel_bounds.plot(ax=ax, edgecolor='black', facecolor='none')\n",
    "        ax.axis('off')\n",
    "        \n",
    "    for ax in (ax1, ax2):\n",
    "        if osm_buildings_parcel is not None:\n",
    "            osm_buildings_parcel.plot(ax=ax, color='blue', alpha=0.3)\n",
    "            \n",
    "    if inferred_buildings_2020_parcel is not None:\n",
    "        inferred_buildings_2020_parcel.plot(ax=ax1, color='red', alpha=0.7)\n",
    "    if title:\n",
    "        ax1.set_title('2020 inferences (raw)')\n",
    "        \n",
    "    if inferred_buildings_2016_parcel is not None:\n",
    "        inferred_buildings_2016_parcel.plot(ax=ax2, color='red', alpha=0.7)\n",
    "    if title:\n",
    "        ax2.set_title('2016 inferences (raw)')\n",
    "        \n",
    "    if osm_parcel is not None:\n",
    "        osm_parcel.plot(ax=ax3, color='blue', alpha=0.7)\n",
    "    if title:\n",
    "        ax3.set_title('OSM annotations')\n",
    "        \n",
    "    if ib_2020_parcel is not None:\n",
    "        ib_2020_parcel.plot(ax=ax4, color='red', alpha=0.7)\n",
    "    if title:\n",
    "        ax4.set_title('OSM-adjusted 2020 polygons')\n",
    "    \n",
    "    if ib_2016_parcel is not None:\n",
    "        ib_2016_parcel.plot(ax=ax5, color='red', alpha=0.7)\n",
    "    if title:\n",
    "        ax5.set_title('OSM-adjusted 2016 polygons')\n",
    "    \n",
    "    # Output\n",
    "    parcel_buildings.plot(ax=ax6, color='purple', alpha=0.7)\n",
    "    if title:\n",
    "        ax6.set_title('Output')\n",
    "    \n",
    "    # Satellite images\n",
    "    if sat_imagery is not None:\n",
    "        \n",
    "        for year, ax in zip(['2020', '2016'], (ax7, ax8)):\n",
    "            # Get imagery\n",
    "            file_name = get_file_name_from_parcel(\n",
    "                parcel_apn, sat_imagery['sj_parcels_res'], sat_imagery['tiles_gdf'][year])\n",
    "            img_file, superres_file = find_image_file_and_superrestile(\n",
    "                sat_imagery['img_fp'][year], sat_imagery['tif_fp'][year], file_name)\n",
    "\n",
    "            with rasterio.open(superres_file) as src:\n",
    "                out_image, out_transform = rasterio.mask.mask(\n",
    "                    src, parcel_bounds.to_crs('EPSG:26910')['geometry'], crop=True, nodata=255)\n",
    "            \n",
    "            # Plot\n",
    "            rasterio.plot.show(out_image, transform=out_transform, ax=ax)\n",
    "    \n",
    "    # Plot independently\n",
    "    if attach is None:\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    return parcel_buildings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb54f4d",
   "metadata": {},
   "source": [
    "## Visualizing satellite imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c94aff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name_from_parcel(parcel_apn, sj_parcels_res, tiles_gdf_year):\n",
    "    # Get parcel bounds\n",
    "    parcel_bounds = sj_parcels_res[sj_parcels_res['APN'] == parcel_apn]\n",
    "    \n",
    "    # Get tiles\n",
    "    tiles = tiles_gdf_year.copy()\n",
    "    \n",
    "    # Return tile with largest overlap with parcel\n",
    "    tiles['iou'] = tiles['geometry'].intersection(parcel_bounds.iloc[0]['geometry']).area\n",
    "    tiles = tiles.sort_values('iou', ascending=False)\n",
    "    tiles = tiles.iloc[0]\n",
    "    \n",
    "    return tiles['file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28439257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_image_file_and_superrestile(img_fp, tif_fp, file_name):\n",
    "    \"\"\"\n",
    "    Searches for the inference file within train, val and test directories.\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.join(img_fp, 'train')):\n",
    "        # For 2020 data which is split across train, val and test\n",
    "        img_file = None\n",
    "        for dirname in ['train', 'val', 'test']:\n",
    "            dirpath = os.path.join(img_fp, dirname, 'images', '{}.npy'.format(file_name))\n",
    "            if os.path.exists(dirpath):\n",
    "                img_file = dirpath\n",
    "    else:\n",
    "        # For 2016 and 2018 data which is not split\n",
    "        img_file = os.path.join(img_fp, '{}.npy'.format(file_name))\n",
    "        \n",
    "    # Generate file \n",
    "    superres_tile = os.path.join(img_fp, '..', 'superres_tif', '{}.tif'.format(file_name))\n",
    "    if not os.path.exists(superres_tile):\n",
    "        if not os.path.exists(os.path.join(img_fp, '..', 'superres_tif')):\n",
    "            os.makedirs(os.path.join(img_fp, '..', 'superres_tif'))\n",
    "        \n",
    "        tile_img = np.load(img_file).astype(np.uint8)\n",
    "        \n",
    "        # Get original raster\n",
    "        raster_original = rasterio.open(os.path.join(tif_fp, '{}.tif'.format(file_name)))\n",
    "        t = from_bounds(*raster_original.bounds, tile_img.shape[0], tile_img.shape[1])\n",
    "        raster_crs = rasterio.crs.CRS({\"init\": \"epsg:26910\"})\n",
    "        \n",
    "        with rasterio.open(superres_tile, 'w', driver='GTiff', \n",
    "                           height=tile_img.shape[0], width=tile_img.shape[1],\n",
    "                           count=3, dtype=str(tile_img.dtype),\n",
    "                           crs=raster_crs, transform=t) as raster_new:\n",
    "            raster_new.write(tile_img[:, :, 0], 1)\n",
    "            raster_new.write(tile_img[:, :, 1], 2)\n",
    "            raster_new.write(tile_img[:, :, 2], 3)\n",
    "            raster_new.close()\n",
    "        \n",
    "    return img_file, superres_tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af980a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_tile_shp(tile_bounds_dict, file_name):\n",
    "#    tile_shp = tile_bounds_dict[file_name]\n",
    "#    \n",
    "#    # Build polygon\n",
    "#    tile_poly = box(bounds[0][0], bounds[0][1], bounds[2][0], bounds[2][1])\n",
    "#    return tile_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c49829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tile_dicts_all_years(oak_fp, inferences_dir):\n",
    "    # Load tile dict for each year\n",
    "    tile_bounds_dict_all = {}\n",
    "    for year in ['2016', '2018', '2020']:\n",
    "        output_fp = os.path.join(oak_fp, 'outputs', 'cbg-inference-{}'.format(year))\n",
    "        with open(os.path.join(output_fp, 'tile_bounds.p'), \"rb\") as f:\n",
    "            tile_bounds_dict = pickle.load(f)\n",
    "            tile_bounds_dict_all[year] = tile_bounds_dict\n",
    "    \n",
    "    # Get tiles for all years\n",
    "    tiles_gdf = {}\n",
    "    for year in ['2016', '2018', '2020']:\n",
    "        tiles = glob.glob(os.path.join(inferences_dir[year], '*.npy'))\n",
    "        tiles = [t.split(os.path.sep)[-1].replace('.npy', '') for t in tiles]\n",
    "        tile_metrics_pd = pd.DataFrame(tiles, columns=['file'])\n",
    "\n",
    "        tile_metrics_pd['geometry'] = tile_metrics_pd.file.progress_apply(\n",
    "            lambda name: tile_bounds_dict_all[year][name] if name in list(tile_bounds_dict_all[year].keys()) else None\n",
    "        )\n",
    "        tiles_gdf[year] = gpd.GeoDataFrame(tile_metrics_pd.copy(), crs='EPSG:4326')\n",
    "        \n",
    "    return tile_bounds_dict_all, tiles_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4015827a",
   "metadata": {},
   "source": [
    "## Ground truth cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e614bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ground_truth_parcels():\n",
    "    # Positive small building constructions\n",
    "    ps_gt_grid = (['23044043', '24960042', '68932067', '27447043', '69414018', \n",
    "                   '23507030', '23016011', '65422047', '48808063', '64902029',\n",
    "                   '24923039', '25430061', '26105043', '47714013', '49124047',\n",
    "                   '30337016', '43406050', '28245062', '29928080', '67032030',\n",
    "                   '43402050', '49904016', '29923105', '49902048', '42914039',\n",
    "                   '43926007', '43917029', '43908005', '49716031', '49709086',\n",
    "                   '43928056', '68404011', '46239027', '69007058', '46423022',\n",
    "                   '67842040', '69433001', '68734049', '70438009', '49124047',\n",
    "                   '48134058', '43408047', '29928060', '57723038', '45915041',\n",
    "                   '59115036', '24956052', '48412033', '43409017', '43939005',\n",
    "                   '46728089'\n",
    "                  ], \n",
    "                  'Positive small build')\n",
    "\n",
    "    # Negative small building constructions\n",
    "    ns_gt_grid = (['24960056', '27406055', '42937040', '47701057', '44249007', \n",
    "                   '40306200', '09218018', '46742046', '27725060', '26434063',\n",
    "                   '41940118', '49709072', '67018055', '44217026', '57736007',\n",
    "                   '42902032', '30334017', '59209020', '24525008', '30502003',\n",
    "                   '46735062', '42927106', '49916006', '29902039', '42108007',\n",
    "                   '42927121', '48116042', '49931042', '67837030', '48134062',\n",
    "                   '43917123', '43415002', '09219023', '48133047', '27717021',\n",
    "                   '25451019', '43926010', '29943040', '48423045', '26126004',\n",
    "                   '48807046', '43915005', '67034072', '24959048', '42927121',\n",
    "                   '42932037', '46425054', '42929005', '23510046', '45603030',\n",
    "                   '56705047', '42913002', '24912099', '24514043', '43931008',\n",
    "                   '43427061', '27413064', '24952032'\n",
    "                  ], \n",
    "                  'Negative small build')\n",
    "\n",
    "    # Positive main building constructions\n",
    "    pm_gt_grid = (['58630050', '42905080', '47202096', '24960042', '48608012', \n",
    "                   '41933001', '24403011', '58616060', '59204040', '25451038', \n",
    "                   '23507030', '23016011', '48443085', '48133141', '46705058',\n",
    "                   '24958056', '26121068', '30334024', '43406050', '29928080',\n",
    "                   '49904016', '42914039', '49445050', '28412012', '42947045',\n",
    "                   '43917029', '43933010', '44244009', '70439060', '58331063',\n",
    "                   '26402037', '43938040', '49709045', '25420163', '41419058',\n",
    "                   '41933001', '70408056', '69430006', '46738031', '67837030',\n",
    "                   '47737008', '44601028', '27909011', '58105018'\n",
    "                  ], \n",
    "                  'Positive main build')\n",
    "\n",
    "    # Negative main building constructions\n",
    "    nm_gt_grid = (['41934035', '49936015', '49722020', '37804025', '44710075', \n",
    "                   '44234038', '46702025', '43944074', '42116035', '24509050', \n",
    "                   '26444013', '70845021', '48809009', '24957040', '28245062',\n",
    "                   '67032030', '43402050', '29923105', '49902048', '42939044',\n",
    "                   '43908005', '49716031', '43929062', '43928056', '46239027',\n",
    "                   '45123132', '69007058', '46423022', '67842040', '41940118',\n",
    "                   '69433001', '68734049', '49145030', '49709072', '67018055',\n",
    "                   '44217026', '23508005', '57736007', '42902032', '30334017',\n",
    "                   '24525008', '30502003', '49124047', '46735062', '26453019',\n",
    "                   '37808013', '42927106', '58303049', '48134058', '67621062',\n",
    "                   '58905026', '43408047', '49916006', '45915091', '29902039',\n",
    "                   '42108007', '24957069', '24945029', '42927121', '70139056',\n",
    "                   '48116042', '49931042', '29928060', '67837030', '43946060',\n",
    "                   '26106017', '43415002', '09219023', '48133047', '43942031',\n",
    "                   '57726030', '57723038', '45915041', '27904040', '46425054',\n",
    "                   '48601061', '48134062', '42924037', '27717021', '25451019',\n",
    "                   '67613018', '65438005', '29943040', '48423045', '26126004',\n",
    "                   '67034072', '42929005', '45603030', '58712047', '68457036',\n",
    "                   '42118043', '70140022', '58616008', '49128017'\n",
    "                  ],\n",
    "                  'Negative main build')\n",
    "    case_dict = {\n",
    "        'Positive small build': ps_gt_grid,\n",
    "        'Negative small build': ns_gt_grid,\n",
    "        'Positive main build': pm_gt_grid,\n",
    "        'Negative main build': nm_gt_grid\n",
    "    }\n",
    "    #for key, val in case_dict.items():\n",
    "    #    print('{}: {} cases'.format(key, len(set(val[0]))))\n",
    "    return case_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cc7052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_parcel_check(parcel_apn, model_params):    \n",
    "    inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, osm_buildings_parcel, permits_parcel = parcel_level_data(parcel_apn, sj_parcels_res, sj_parcel_permit)\n",
    "    parcel_bounds = sj_parcels_res[sj_parcels_res['APN'] == parcel_apn]\n",
    "\n",
    "    # Incorporate OSM data\n",
    "    parcel_buildings, _ = process_OSM_data(\n",
    "        inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, \n",
    "        osm_buildings_parcel, parcel_bounds, model_params)\n",
    "        \n",
    "    # Assess\n",
    "    main_build = parcel_buildings.loc[parcel_buildings['main_building_flag'] == True].iloc[0]\n",
    "    main_build_expansion = main_build['expansion_2016_flag']\n",
    "\n",
    "    small_build = parcel_buildings.loc[parcel_buildings['main_building_flag'] != True]\n",
    "    small_build_expansion = small_build['expansion_2016_flag'].sum()\n",
    "    \n",
    "    test = {\n",
    "        'Negative small build': (small_build_expansion == 0),\n",
    "        'Positive small build': (small_build_expansion > 0),\n",
    "        'Positive main build': main_build_expansion,\n",
    "        'Negative main build': not main_build_expansion\n",
    "    }\n",
    "\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84d1dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_model_check(model_params, visualize=False):\n",
    "    \"\"\"\n",
    "    visualize: (bool) visualizes ground truth parcels that model failed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load ground truth cases\n",
    "    case_dict = load_ground_truth_parcels()\n",
    "    test_df = pd.DataFrame(columns=['parcel_apn', 'gt_type', 'pass'])\n",
    "    \n",
    "    t0 = time.time()\n",
    "    for gt_key, (gt_parcels, _) in tqdm(case_dict.items()):\n",
    "        for parcel_apn in gt_parcels:\n",
    "            parcel_test = ground_truth_parcel_check(\n",
    "                parcel_apn=parcel_apn,\n",
    "                model_params=model_params)\n",
    "            \n",
    "            # Select test associated to the parcel ground truth\n",
    "            parcel_test = parcel_test[gt_key]\n",
    "            \n",
    "            # Append\n",
    "            test_df = pd.concat([\n",
    "                test_df, pd.DataFrame.from_dict(\n",
    "                    {'parcel_apn': [parcel_apn], 'gt_type': [gt_key], 'pass': [parcel_test]})])\n",
    "\n",
    "    # Compute overall metrics\n",
    "    print('Time: {}'.format(time.time() - t0))\n",
    "    results_df = pd.DataFrame()\n",
    "    for key in case_dict.keys():\n",
    "        key_df = test_df.loc[test_df['gt_type'] == key]\n",
    "        results_df = pd.concat([\n",
    "            results_df, pd.DataFrame.from_dict({'ground_truth_type': [key], \n",
    "                                                'num_pass': [key_df['pass'].sum()], \n",
    "                                                'num_total': [len(key_df)]\n",
    "                                               })])\n",
    "        print('{}: {}/{}; {}%'.format(key, key_df['pass'].sum(), len(key_df), 100 * key_df['pass'].mean()))\n",
    "    \n",
    "    # Visualize failed parcels\n",
    "    if test_df['pass'].sum() == len(test_df):\n",
    "        print('Model passes all tests')\n",
    "    else:\n",
    "        test_df_failed = test_df.loc[test_df['pass'] == 0]\n",
    "\n",
    "        for key in tqdm(case_dict.keys()):\n",
    "            key_df = test_df_failed.loc[test_df_failed['gt_type'] == key]\n",
    "            if len(key_df) > 0:\n",
    "                if visualize:\n",
    "                    print('Failed {} cases'.format(key))\n",
    "                viz_parcel_apns = key_df['parcel_apn'].unique()\n",
    "                for parcel_apn in viz_parcel_apns:\n",
    "                    if visualize:\n",
    "                        run_osm_apn_full_detail(\n",
    "                            parcel_apn=parcel_apn, model_params=model_params, \n",
    "                            sat_imagery=None, attach=None, title=True)\n",
    "    return results_df, test_df_failed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c258eb",
   "metadata": {},
   "source": [
    "# Generic pipeline (population estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d423a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_parcel_buildings(inferred_buildings_parcel, osm_buildings_parcel, \n",
    "                             parcel_bounds, model_params):\n",
    "    \"\"\"\n",
    "    Integrates OSM and model inferences to define the main and small buildings for each parcel, \n",
    "    and the associated confidence level of each building.\n",
    "    param: inferred_buildings_parcel: (dict) of inference gpd.GeoDataFrames for each year to be processed\n",
    "    param: osm_buildings_parcel: (dict) of OSM gpd.GeoDataFrames for each year to be processed\n",
    "    param: parcel_bounds: (shp)\n",
    "    param: model_params: (dict) including parameters for processing specifications\n",
    "    \n",
    "    returns: (dict) of processed building gpd.GeoDataFrames for each year. Columns include:\n",
    "        - geometry, main_building_flag, OSM_flag, confidence_level, area\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Check model params\n",
    "    param_keys = ['area_threshold_main', 'area_threshold_small', \n",
    "                  'main_expansion_type', 'main_polygon_definition',\n",
    "                  'negative_buffer', 'simplify_tolerance', 'flatten_threshold']\n",
    "    for param in param_keys:\n",
    "        assert param in list(model_params.keys())\n",
    "    assert model_params['main_expansion_type'] in ['raw_polygons', 'MRR', 'convex_hull', \n",
    "                                                   'osm_union', 'raw_polygons_iou']\n",
    "    assert model_params['main_polygon_definition'] in ['Largest', 'OSM']\n",
    "    \n",
    "    # Check years to be processed\n",
    "    assert set(inferred_buildings_parcel.keys()) == set(osm_buildings_parcel.keys())\n",
    "    years = inferred_buildings_parcel.keys()\n",
    "    \n",
    "    # Define columns and output GDFs\n",
    "    gpd_cols = ['main_building_flag', 'OSM_flag', 'build_confidence', 'geometry']\n",
    "    \n",
    "    parcel_buildings = {}\n",
    "    data_dict = {}\n",
    "    for year in years:\n",
    "        parcel_buildings[year] = gpd.GeoDataFrame(geometry=[], columns=gpd_cols)\n",
    "    \n",
    "    # Identify main buildings\n",
    "    parcel_builds, parcel_main_geoms = identify_parcel_main_buildings(\n",
    "        inferred_buildings_parcel, osm_buildings_parcel, parcel_bounds, model_params)\n",
    "    \n",
    "    # Process the inferences for each year\n",
    "    for year in years:\n",
    "        dict_builds = parcel_builds[year]\n",
    "        dict_main_geom = parcel_builds[year]\n",
    "        \n",
    "        # Adjust inferences and compute confidence\n",
    "        adj_inference_parcel = adjust_buildings(\n",
    "            inference=dict_builds['inference'], osm=dict_builds['osm'], \n",
    "            model_params=model_params, limit_to_inferences=False)\n",
    "        \n",
    "        # Compute building area\n",
    "        if adj_inference_parcel is not None:\n",
    "            adj_inference_parcel['area'] = adj_inference_parcel.to_crs('EPSG:26910').geometry.area\n",
    "    \n",
    "        # Generate data dict (for debugging)\n",
    "        data_dict[year] = {'inference': adj_inference_parcel, 'osm': dict_builds['osm']}\n",
    "        \n",
    "        # Add to main output\n",
    "        if adj_inference_parcel is not None:\n",
    "            adj_inference_parcel = adj_inference_parcel[gpd_cols + ['area']]\n",
    "        parcel_buildings[year] = adj_inference_parcel\n",
    "    \n",
    "    return parcel_buildings, data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1484cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou_gdfs(base, comp, col_name):\n",
    "    if 'index_right' in base.columns:\n",
    "        base.drop('index_right', axis=1, inplace=True)\n",
    "    \n",
    "    comp.reset_index(inplace=True, drop=True)\n",
    "    base = base.sjoin(comp[['geometry']], how='left', predicate='intersects')\n",
    "        \n",
    "    base[col_name] = base.apply(\n",
    "        lambda row: 0 if pd.isnull(row['index_right']) else row['geometry'].intersection(\n",
    "            comp.iloc[int(row['index_right'])]['geometry']).area/row['geometry'].union(comp.iloc[\n",
    "            int(row['index_right'])]['geometry']).area, axis=1)\n",
    "    \n",
    "    # Handle multiple matches\n",
    "    base.sort_values(col_name, ascending=False, inplace=True)\n",
    "    base.drop_duplicates(subset=['geometry'], keep='first', inplace=True)\n",
    "    \n",
    "    base.drop('index_right', axis=1, inplace=True)\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e1126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_buildings(inference, osm, model_params, limit_to_inferences):\n",
    "    match_cols = ['GEOID', 'area', 'main_building_flag', 'OSM_flag', 'build_confidence', 'geometry'] \n",
    "    \n",
    "    if inference is None and osm is None:\n",
    "        return None\n",
    "    \n",
    "    if inference is None:\n",
    "        # Use OSM directly and assign confidence = 0\n",
    "        inference = osm.copy()\n",
    "        inference['OSM_flag'] = True\n",
    "        inference['build_confidence'] = 0\n",
    "        return inference\n",
    "    \n",
    "    if osm is None:\n",
    "        inference['build_confidence'] = 0\n",
    "        return inference\n",
    "    \n",
    "    # * SMALL BUILDINGS * -------------------------------------------\n",
    "    \n",
    "    # Default to OSM for all buildings except main building\n",
    "    inference_small_build = inference.loc[inference['main_building_flag'] == False]\n",
    "    osm_small_build = osm.loc[osm['main_building_flag'] == False].copy()\n",
    "    \n",
    "    # Check for missing buildings (\"false positives\") in OSM annotations\n",
    "    osm_small_build_union = osm_small_build.geometry.unary_union\n",
    "    \n",
    "    fp = inference_small_build.copy()\n",
    "    if len(fp) > 0:\n",
    "        fp['inf_not_covered'] = fp.apply(\n",
    "            lambda row: compare_building_footprint(\n",
    "                base_geom=osm_small_build_union, \n",
    "                new_geom=row['geometry'], \n",
    "                diff_type='protruding_poly', \n",
    "                model_params=model_params, \n",
    "                main_building_flag=row['main_building_flag'])[0],\n",
    "            axis=1)\n",
    "\n",
    "        fp = fp.sjoin(osm_small_build[['geometry']], how='left')\n",
    "        fp = fp.loc[(fp['inf_not_covered'] == True) | (fp['index_right'].isna())]\n",
    "\n",
    "        # Account for multiple OSM matches\n",
    "        fp.drop_duplicates(subset=['geometry'], inplace=True)\n",
    "    fp['OSM_flag'] = False\n",
    "    \n",
    "    # Compute IoU between OSM small build and model small build\n",
    "    osm_small_build = compute_iou_gdfs(osm_small_build, inference_small_build, 'build_confidence')\n",
    "    fp = compute_iou_gdfs(fp, osm_small_build, 'build_confidence')\n",
    "    \n",
    "    # Keep only buildings identified in inferences (we're lenient and allow for anything)\n",
    "    # that is at least 30% covered by the inferences to be included or \n",
    "    # inference footprint is 60% covered by OSM.\n",
    "\n",
    "    if limit_to_inferences and len(osm_small_build) > 0:\n",
    "        raise Exception('[ERROR] Not implemented')\n",
    "    #    # Cover 30% of OSM footprints\n",
    "    #    gdf_geom = inference.geometry.unary_union\n",
    "    #    parcel_build['osm_coverage'] = parcel_build['geometry'].intersection(\n",
    "    #        gdf_geom).area/parcel_build['geometry'].area\n",
    "        \n",
    "        # OSM covers 60% of inference footprint\n",
    "    #    inference.reset_index(inplace=True, drop=True)\n",
    "    #    parcel_build = parcel_build.sjoin(inference[['geometry']], how='left', predicate='intersects')\n",
    "        \n",
    "    #    parcel_build['inf_coverage'] = parcel_build.apply(\n",
    "    #        lambda row: 0 if pd.isnull(row['index_right']) else row['geometry'].intersection(\n",
    "    #            inference.iloc[int(row['index_right'])]['geometry']).area/inference.iloc[\n",
    "    #            int(row['index_right'])]['geometry'].area, axis=1)\n",
    "        \n",
    "    #    parcel_build = parcel_build.loc[(parcel_build['osm_coverage'] > 0.3) | (\n",
    "    #        parcel_build['inf_coverage'] > 0.6)]\n",
    "        \n",
    "    # Concatenate OSM small buildings and small buildings missed by OSM\n",
    "    parcel_build = pd.concat([osm_small_build[match_cols], fp[match_cols]])\n",
    "    \n",
    "    # * MAIN BUILDING * -------------------------------------------\n",
    "    inference_main_build = inference.loc[inference['main_building_flag'] == True]\n",
    "    osm_main_build = osm.loc[osm['main_building_flag'] == True].copy()\n",
    "    \n",
    "    inference_main_build = compute_iou_gdfs(inference_main_build, osm_main_build, 'build_confidence')\n",
    "    \n",
    "    # Add back main building\n",
    "    parcel_build = pd.concat([inference_main_build[match_cols], parcel_build[match_cols]])\n",
    "    \n",
    "    # Flatten\n",
    "    inference = flatten_geometries(gdf=parcel_build, model_params=model_params)\n",
    "\n",
    "    return inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8109e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_parcel_main_buildings(inferred_buildings_parcel_dict, osm_buildings_parcel_dict, \n",
    "                            parcel_bounds, model_params):\n",
    "    \n",
    "    # Check years\n",
    "    assert set(inferred_buildings_parcel_dict.keys()) == set(osm_buildings_parcel_dict.keys())\n",
    "    \n",
    "    parcel_builds = {}\n",
    "    parcel_main_geoms = {}\n",
    "    \n",
    "    for year in inferred_buildings_parcel_dict.keys():\n",
    "        \n",
    "        osm_buildings_parcel = osm_buildings_parcel_dict[year]\n",
    "        inferred_buildings_parcel = inferred_buildings_parcel_dict[year]\n",
    "        \n",
    "        # Identify OSM main building\n",
    "        osm_main_build_geom, osm_main_build = None, None\n",
    "\n",
    "        if osm_buildings_parcel is not None:\n",
    "            # Filter duplicate geometries\n",
    "            osm_buildings_parcel.drop_duplicates(subset=['geometry'], inplace=True)\n",
    "\n",
    "            osm_buildings_parcel = osm_buildings_parcel.sort_values('area', ascending=False)\n",
    "\n",
    "            # Identify main building\n",
    "            osm_buildings_parcel = osm_buildings_parcel.reset_index(drop=True)\n",
    "            osm_buildings_parcel['main_building_flag'] = osm_buildings_parcel.apply(\n",
    "                lambda row: True if row.name == 0 else False, axis=1)\n",
    "\n",
    "            osm_buildings_parcel['OSM_flag'] = True\n",
    "            osm_main_build_geom = osm_buildings_parcel.iloc[0]['geometry']\n",
    "\n",
    "        # Identify main inference building\n",
    "        inferred_buildings_main_geom = None\n",
    "        \n",
    "        #print('before')\n",
    "        #inferred_buildings_parcel.plot()\n",
    "        #plt.show()\n",
    "        \n",
    "        if inferred_buildings_parcel is not None:\n",
    "            # Clip inferences\n",
    "            inferred_buildings_parcel = gpd.clip(inferred_buildings_parcel, parcel_bounds)\n",
    "\n",
    "            inferred_buildings_parcel = inferred_buildings_parcel.reset_index(drop=True)\n",
    "            inferred_buildings_parcel = get_inference_main_building(\n",
    "                inferred_buildings_parcel, osm_buildings_parcel, model_params)\n",
    "\n",
    "            inferred_buildings_main_geom = inferred_buildings_parcel.iloc[0]['geometry'] \n",
    "            \n",
    "        parcel_builds[year] = {'inference': inferred_buildings_parcel, 'osm': osm_buildings_parcel}\n",
    "        parcel_main_geoms[year] = {'inference': inferred_buildings_main_geom, 'osm': osm_main_build_geom}\n",
    "\n",
    "    return parcel_builds, parcel_main_geoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e240562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_processed_parcel(parcel_apn, model_params, sat_imagery=None, attach=None, title=True):\n",
    "    inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, osm_buildings_parcel, permits_parcel = parcel_level_data(parcel_apn, sj_parcels_res, sj_parcel_permit)\n",
    "    parcel_bounds = sj_parcels_res[sj_parcels_res['APN'] == parcel_apn]\n",
    "\n",
    "    # Prepare data\n",
    "    inferred_buildings_parcel = {'2016': inferred_buildings_2016_parcel,\n",
    "                                 '2020': inferred_buildings_2020_parcel}\n",
    "    \n",
    "    osm_buildings_parcel = {'2016': osm_buildings_parcel,\n",
    "                            '2020': osm_buildings_parcel}\n",
    "    print('[INFO] OSM HISTORIC DATA NOT YET INCORPORATED')\n",
    "    \n",
    "    # Incorporate OSM data\n",
    "    parcel_buildings, data_dict = process_parcel_buildings(\n",
    "        inferred_buildings_parcel, osm_buildings_parcel, parcel_bounds, model_params)\n",
    "\n",
    "    ib_2020_parcel = data_dict['2020']['inference']\n",
    "    ib_2016_parcel = data_dict['2016']['inference']\n",
    "    osm_2020_parcel = data_dict['2020']['osm']\n",
    "    osm_2016_parcel = data_dict['2016']['osm']\n",
    "    \n",
    "    # Define axis -- depends on whether we want an independent plot, or to attach to another plot\n",
    "    if attach is None:\n",
    "        if sat_imagery is None:\n",
    "            fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))\n",
    "        else:\n",
    "            fig, axs = plt.subplots(ncols=2, nrows=3, figsize=(8, 8))\n",
    "    else:\n",
    "        fig, axs = attach\n",
    "\n",
    "    # Get individual axis -- number depends on whether we visualize satellite imagery\n",
    "    if sat_imagery is None:\n",
    "        (ax1, ax2), (ax3, ax4) = axs\n",
    "    else:\n",
    "        (ax1, ax2), (ax3, ax4), (ax5, ax6) = axs\n",
    "\n",
    "    for ax in [a for alist in axs for a in alist]:\n",
    "        parcel_bounds.plot(ax=ax, edgecolor='black', facecolor='none')\n",
    "        ax.axis('off')\n",
    "        \n",
    "    # * OSM\n",
    "    if osm_2020_parcel is not None:\n",
    "        osm_2020_parcel.plot(ax=ax1, color='blue', alpha=0.3)\n",
    "        \n",
    "    if osm_2016_parcel is not None:\n",
    "        osm_2016_parcel.plot(ax=ax2, color='blue', alpha=0.3)\n",
    "    \n",
    "    # * Pre-processed Inferences\n",
    "    if inferred_buildings_2020_parcel is not None:\n",
    "        inferred_buildings_2020_parcel.plot(ax=ax1, color='red', alpha=0.7)\n",
    "\n",
    "    if inferred_buildings_2016_parcel is not None:\n",
    "        inferred_buildings_2016_parcel.plot(ax=ax2, color='red', alpha=0.7)\n",
    "        \n",
    "    # Output\n",
    "    parcel_buildings['2020'].plot(ax=ax3, color='purple', alpha=0.7)\n",
    "    parcel_buildings['2016'].plot(ax=ax4, color='purple', alpha=0.7)\n",
    "    \n",
    "    # Add titles\n",
    "    if title:\n",
    "        ax1.set_title('2020 inferences (raw)')\n",
    "        ax2.set_title('2016 inferences (raw)')\n",
    "        ax3.set_title('OSM-adjusted 2020 polygons')\n",
    "        ax4.set_title('OSM-adjusted 2016 polygons')\n",
    "    \n",
    "    # Satellite images\n",
    "    if sat_imagery is not None:\n",
    "        \n",
    "        for year, ax in zip(['2020', '2016'], (ax5, ax6)):\n",
    "            # Get imagery\n",
    "            file_name = get_file_name_from_parcel(\n",
    "                parcel_apn, sat_imagery['sj_parcels_res'], sat_imagery['tiles_gdf'][year])\n",
    "            img_file, superres_file = find_image_file_and_superrestile(\n",
    "                sat_imagery['img_fp'][year], sat_imagery['tif_fp'][year], file_name)\n",
    "\n",
    "            with rasterio.open(superres_file) as src:\n",
    "                out_image, out_transform = rasterio.mask.mask(\n",
    "                    src, parcel_bounds.to_crs('EPSG:26910')['geometry'], crop=True, nodata=255)\n",
    "            \n",
    "            # Plot\n",
    "            rasterio.plot.show(out_image, transform=out_transform, ax=ax)\n",
    "    \n",
    "    # Plot independently\n",
    "    if attach is None:\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    return parcel_buildings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ef48de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parcel_confidence(parcel_apn, model_params):\n",
    "    \"\"\"\n",
    "    Returns a measure of confidence in [0, 1] that aggregates small building-level\n",
    "    confidence spatially and temporally at the parcel level. For parcels that do not\n",
    "    have small buildings (in any year), we return None as these get allocated to a\n",
    "    separate bin.\n",
    "    :param parcel_apn: (str) Parcel APN\n",
    "    :param model_params: (dict) parameters for the polygonization pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    parcel_items = parcel_level_data(parcel_apn, sj_parcels_res, sj_parcel_permit)\n",
    "    inferred_buildings_2020_parcel, inferred_buildings_2016_parcel, osm_buildings_parcel, permits_parcel = parcel_items\n",
    "    \n",
    "    parcel_bounds = sj_parcels_res[sj_parcels_res['APN'] == parcel_apn]\n",
    "\n",
    "    # Prepare data\n",
    "    inferred_buildings_parcel = {'2016': inferred_buildings_2016_parcel,\n",
    "                                 '2020': inferred_buildings_2020_parcel}\n",
    "    \n",
    "    osm_buildings_parcel = {'2016': osm_buildings_parcel,\n",
    "                            '2020': osm_buildings_parcel}\n",
    "    #print('[INFO] OSM HISTORIC DATA NOT YET INCORPORATED')\n",
    "    \n",
    "    # Incorporate OSM data\n",
    "    parcel_buildings, _ = process_parcel_buildings(\n",
    "        inferred_buildings_parcel, osm_buildings_parcel, parcel_bounds, model_params)\n",
    "\n",
    "    # Compute confidence\n",
    "    confidence = []\n",
    "    for year in ['2016', '2020']:\n",
    "        if parcel_buildings[year] is None:\n",
    "            confidence.append(None)\n",
    "            continue\n",
    "        \n",
    "        small_builds = parcel_buildings[year].copy()\n",
    "        small_builds = small_builds.loc[small_builds['main_building_flag'] == False]\n",
    "        \n",
    "        # Parcels with no small buildings do not get a measure of confidence -- these get\n",
    "        # allocated to a separate bin. Identify them using None.\n",
    "        if len(small_builds) == 0:\n",
    "            confidence.append(None)\n",
    "        # Aggregate spatially\n",
    "        else:\n",
    "            confidence.append(small_builds['build_confidence'].mean())\n",
    "        \n",
    "    # Aggregate temporally\n",
    "    if None in confidence:\n",
    "        return None, parcel_buildings\n",
    "    return np.mean(confidence), parcel_buildings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
